{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YT9EMPJ2-Z1"
   },
   "source": [
    "# SK-Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "XOCSJ2N027YC",
    "outputId": "cf7a7c37-90b0-4d30-85e2-62ae2fdd0dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n",
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)\n",
    "\n",
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "\n",
    "skl_output.shape\n",
    "\n",
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])\n",
    "\n",
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zV-KJXeQ3FZI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOmYQLaU3YL5"
   },
   "source": [
    "# Your custom implementation: \n",
    "# Task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "xSgm8l6K3ZqX"
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# Make sure its well documented and readble with appropriate comments.\n",
    "# Compare your results with the above sklearn tfidf vectorizer\n",
    "# You are not supposed to use any other library apart from the ones given below\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "## SkLearn# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "3wKimplN3b2x"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# fit method is used to identify the unique words in the corpus and add dimension to it in the dictionary-format.\n",
    "def fit(data):\n",
    "  if type(data) == type(list()):\n",
    "      s = set()\n",
    "      for i in range(len(data)): # Iterating over every row in the corpus and finding the unique words of (length > 2)\n",
    "          x = data[i].split()\n",
    "          for j in range(len(x)):\n",
    "              if len(x[j]) < 2:\n",
    "                  continue\n",
    "              s.add(x[j])\n",
    "      d = {j:i for i,j in enumerate(sorted(s))} # d : ( keys = unique-words ) and (values = dimension-number)\n",
    "      return d\n",
    "  else:\n",
    "        print(\"you need to pass list of sentance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jePQvZAfAQb0"
   },
   "outputs": [],
   "source": [
    "def transform(corpus,vocab):\n",
    "  idf_dict = dict() # keys = unique-words , values = number of documents contain the corresponding unique-word.\n",
    "  \n",
    "  idf_ = list() # used for printing the idf values\n",
    "  \n",
    "  for word in vocab.keys(): # this for-loop is used to find number of documents contain the corresponding unique-word.\n",
    "    c=0\n",
    "    for row in (corpus):\n",
    "      if word in row:\n",
    "        c+=1\n",
    "    idf_dict[word] = c\n",
    "    idf_.append(1+ math.log((1+len(corpus))/(c+1)))\n",
    "  print('\\n',idf_)\n",
    "  rows = []\n",
    "  cols = []\n",
    "  vals = []\n",
    "  print('\\n','*'*50)\n",
    "  for indx,row in enumerate(corpus):\n",
    "    a = dict(Counter(row.split()))\n",
    "    for word,freq in a.items():\n",
    "      col_indx = vocab.get(word,-1)\n",
    "      if col_indx != -1:\n",
    "        rows.append(indx)\n",
    "        cols.append(col_indx)\n",
    "        tf = freq / sum(a.values())\n",
    "        idf = 1 + (math.log((1+len(corpus))/(1 + idf_dict[word])))\n",
    "        res = (tf * idf)\n",
    "        #print(tf,'*',idf,'=',res)\n",
    "        vals.append(res)\n",
    "  b = csr_matrix((vals, (rows,cols)), shape=(len(corpus),len(vocab)))\n",
    "  b = normalize(b,norm='l2')\n",
    "  return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "sCsTqNv_KYTa",
    "outputId": "229d78cc-9eae-4e1d-abc1-e3c28e872010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      " [1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n",
      "\n",
      " **************************************************\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "vocab = fit(corpus)\n",
    "print(list(vocab.keys()))\n",
    "print(transform(corpus,vocab).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "2crwbzyFSFhV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "1PhYQPKJAQwL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzLYtZyDARHw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0W1fXQuARc0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75FjVWowbFaO"
   },
   "source": [
    "# Task-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vgwrFJCHARSG"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "corpus = pickle.load(open(\"cleaned_strings\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "GlYxxhLXAQ9I"
   },
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "  if type(data) == type(list()):\n",
    "      s = set()\n",
    "      ''' \n",
    "      Finding the unique words in the corpus\n",
    "      '''\n",
    "      for i in range(len(data)):\n",
    "          x = data[i].split()\n",
    "          for j in range(len(x)):\n",
    "              if len(x[j]) < 2:\n",
    "                  continue\n",
    "              s.add(x[j])\n",
    "\n",
    "      d1 = dict() # d1: is used to store the unique-words as keys and ( values = number of documents contains this word )\n",
    "      fit.d2 = dict() # d2: ( keys = unique-words) and (values = Inverse-document-freq values) in decending order of values\n",
    "    \n",
    "      for word in s: # this for-loop is used to find number of documents contain the corresponding unique-word.\n",
    "        c=0\n",
    "        for row in (data):\n",
    "          if word in row:\n",
    "            c+=1\n",
    "        d1[word] = c\n",
    "        fit.d2[word] = 1 + (math.log((1+len(data)) / (1 + d1[word])))\n",
    "      s1 = (sorted(fit.d2.items(), key=lambda x:x[1],reverse=True)[:50]) # top 50 words based on idf scores.\n",
    "     # s1 = sorted(s1) # since the idf-scores for top-50 words are same, so sorted these top-50 words in alphabetical order.\n",
    "      #print(s1)\n",
    "      s2 = [i[0] for i in s1]\n",
    "      d = {j:i for i,j in enumerate((s2))}\n",
    "      return d\n",
    "  else:\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ueOmc4Ff8fgs"
   },
   "outputs": [],
   "source": [
    "def transform(corpus,vocab):\n",
    "  rows = []\n",
    "  cols = []\n",
    "  vals = []\n",
    "  for indx,row in enumerate(corpus):\n",
    "    a = dict(Counter(row.split()))\n",
    "    for word,freq in a.items():\n",
    "      col_indx = vocab.get(word,-1)\n",
    "      if col_indx != -1:\n",
    "        rows.append(indx)\n",
    "        cols.append(col_indx)\n",
    "        tf = freq / sum(a.values())\n",
    "        idf = fit.d2[word] # using the same dictionary (fit.d2) from fit method where ( keys = unique-words) and (values = Inverse-document-freq scores) in decending order of values \n",
    "        #print(idf)\n",
    "        res = float(tf * idf)\n",
    "        #print(tf,'*',idf,'=',res)\n",
    "        vals.append(res)\n",
    "  b = csr_matrix((vals, (rows,cols)), shape=(len(corpus),len(vocab)))\n",
    "  b = normalize(b,norm='l2')\n",
    "  return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "mmxNQLaYO_C7",
    "outputId": "e0dc76b3-36a4-4074-9b46-c8e36bc615c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['holding', 'shameful', 'landscapes', 'removing', 'secondary', 'revenge', 'politically', 'repeating', 'massive', 'cliff', 'kathy', 'rendering', 'virus', 'hayworth', 'fire', 'cutie', 'fanciful', 'reporter', 'boss', 'represents', 'sounded', 'regardless', 'portrayed', 'angelina', 'spy', 'quaid', 'applause', 'shell', 'drawn', 'angela', 'voyage', 'evidently', 'timing', 'truth', 'unlockable', 'smith', 'menacing', 'edward', 'murdering', 'merit', 'selections', 'females', 'recover', 'pledge', 'flicks', 'finest', 'washed', 'manages', 'colours', 'discovery']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "vocab = fit(corpus)\n",
    "print(list(vocab.keys()))\n",
    "print(transform(corpus,vocab).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "JkyqwewH8fD2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "YfydxBzU8eZ1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "c9En2j_pAQRn",
    "outputId": "a6740619-5da8-4bbb-b612-d4834686d61c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nWThBrciAOwQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "hEJsJl-P8a5G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "m685h3URb6Sq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6rw6ikrhnh5l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vnhJHCO-60in"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "DW0yoAAB-wkj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-QaJxqZIFtC0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "264do-T6Kf0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "mCDC1XM-8dVh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "nFw7J-GG8guj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A-3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
