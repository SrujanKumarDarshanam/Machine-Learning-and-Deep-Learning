{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQzFIilmEJ05"
   },
   "source": [
    "## Assignment : 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF8r9ph1EQra",
    "outputId": "fe9d14b5-519b-4d37-f6d3-4eb77773c296"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bpw-jg3CEJ08"
   },
   "source": [
    "<pre>\n",
    "1. You can work with preprocessed_data.csv for the assignment. You can get the data from - <a href='https://drive.google.com/drive/u/0/folders/1CJnItndeSSJu7aragQoXWZS9-0apN6pp'>Data folder </a>\n",
    "2. Load the data in your notebook.\n",
    "3. After step 2 you have to train 3 types of models as discussed below. \n",
    "4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a  href='https://stackoverflow.com/a/46844409'>this</a> and <a  href='https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807'>this</a> for using auc as a metric \n",
    "5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n",
    "6. You can use any one of the optimizers and choice of Learning rate and momentum.\n",
    "7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in a separate pad and write your observations about them.\n",
    "8. Make sure that you are using GPU to train the given models.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nEIVj8VEJ0-"
   },
   "outputs": [],
   "source": [
    "#you can use gdown modules to import dataset for the assignment\n",
    "#for importing any file from drive to Colab you can write the syntax as !gdown --id file_id\n",
    "#you can run the below cell to import the required preprocessed data.csv file and glove vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPTqtK8FEJ0_"
   },
   "outputs": [],
   "source": [
    "#!gdown --id 1GpATd_pM4mcnWWIs28-s1lgqdAg2Wdv-\n",
    "#!gdown --id 1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIg321uiEJ0_"
   },
   "source": [
    "## <font color='red'> Model-1 </font>\n",
    "Build and Train deep neural network as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMdK29ICEJ1A"
   },
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>\n",
    "ref: https://i.imgur.com/w395Yk9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGVOTmZFEJ1A"
   },
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1kfO7lVEJ1A"
   },
   "source": [
    "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUM79kbiEJ1B"
   },
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "input_layer = Input(shape=(n,))\n",
    "embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n",
    "flatten = Flatten()(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpI9uRQ8EJ1B"
   },
   "source": [
    "### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMi_xYIwEJ1B"
   },
   "source": [
    "# <font color='red'> Model-1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xaFiR41AEJ1C"
   },
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "#make sure that you import your libraries from tf.keras and not just keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bd_RcF3kEJ1C"
   },
   "outputs": [],
   "source": [
    "#read the csv file\n",
    "import pandas as pd\n",
    "p1 = '/content/drive/MyDrive/AAIC/Assignments/LSTM on Donors Choose/preprocessed_data_final.csv'\n",
    "p2 = \"C:/Users/darsh/Downloads/Srujan/Donars Choose Assignment/preprocessed_data_final.csv\"\n",
    "df = pd.read_csv(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "cf68m3J5EJ1C",
    "outputId": "c9b0bd13-f185-4816-fa6b-affa2c78f33e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>resource_summary_contains_numerical_digits</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>essay</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_resource_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "      <td>in</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>esl_literacy</td>\n",
       "      <td>mrs</td>\n",
       "      <td>0</td>\n",
       "      <td>students english learners working english seco...</td>\n",
       "      <td>Educational Support for English Learners at Home</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "      <td>fl</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>history_civics_health_sports</td>\n",
       "      <td>civics_government_teamsports</td>\n",
       "      <td>mr</td>\n",
       "      <td>1</td>\n",
       "      <td>students arrive school eager learn polite gene...</td>\n",
       "      <td>Wanted: Projector for Hungry Learners</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "      <td>az</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>health_sports</td>\n",
       "      <td>health_wellness_teamsports</td>\n",
       "      <td>ms</td>\n",
       "      <td>0</td>\n",
       "      <td>true champions not always ones win guts mia ha...</td>\n",
       "      <td>Soccer Equipment for AWESOME Middle School Stu...</td>\n",
       "      <td>My students need shine guards, athletic socks,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>232.90</td>\n",
       "      <td>4</td>\n",
       "      <td>ky</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>literacy_language_math_science</td>\n",
       "      <td>literacy_mathematics</td>\n",
       "      <td>mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>work unique school filled esl english second l...</td>\n",
       "      <td>Techie Kindergarteners</td>\n",
       "      <td>My students need to engage in Reading and Math...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67.98</td>\n",
       "      <td>4</td>\n",
       "      <td>tx</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>math_science</td>\n",
       "      <td>mathematics</td>\n",
       "      <td>mrs</td>\n",
       "      <td>1</td>\n",
       "      <td>second grade classroom next year made around 2...</td>\n",
       "      <td>Interactive Math Tools</td>\n",
       "      <td>My students need hands on practice in mathemat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   teacher_number_of_previously_posted_projects  \\\n",
       "0                                             0   \n",
       "1                                             7   \n",
       "2                                             1   \n",
       "3                                             4   \n",
       "4                                             1   \n",
       "\n",
       "   resource_summary_contains_numerical_digits   price  quantity school_state  \\\n",
       "0                                           0  154.60        23           in   \n",
       "1                                           0  299.00         1           fl   \n",
       "2                                           0  516.85        22           az   \n",
       "3                                           0  232.90         4           ky   \n",
       "4                                           0   67.98         4           tx   \n",
       "\n",
       "  project_grade_category                clean_categories  \\\n",
       "0          grades_prek_2               literacy_language   \n",
       "1             grades_6_8    history_civics_health_sports   \n",
       "2             grades_6_8                   health_sports   \n",
       "3          grades_prek_2  literacy_language_math_science   \n",
       "4          grades_prek_2                    math_science   \n",
       "\n",
       "            clean_subcategories teacher_prefix  project_is_approved  \\\n",
       "0                  esl_literacy            mrs                    0   \n",
       "1  civics_government_teamsports             mr                    1   \n",
       "2    health_wellness_teamsports             ms                    0   \n",
       "3          literacy_mathematics            mrs                    1   \n",
       "4                   mathematics            mrs                    1   \n",
       "\n",
       "                                               essay  \\\n",
       "0  students english learners working english seco...   \n",
       "1  students arrive school eager learn polite gene...   \n",
       "2  true champions not always ones win guts mia ha...   \n",
       "3  work unique school filled esl english second l...   \n",
       "4  second grade classroom next year made around 2...   \n",
       "\n",
       "                                       project_title  \\\n",
       "0   Educational Support for English Learners at Home   \n",
       "1              Wanted: Projector for Hungry Learners   \n",
       "2  Soccer Equipment for AWESOME Middle School Stu...   \n",
       "3                             Techie Kindergarteners   \n",
       "4                             Interactive Math Tools   \n",
       "\n",
       "                            project_resource_summary  \n",
       "0  My students need opportunities to practice beg...  \n",
       "1  My students need a projector to help with view...  \n",
       "2  My students need shine guards, athletic socks,...  \n",
       "3  My students need to engage in Reading and Math...  \n",
       "4  My students need hands on practice in mathemat...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ob8qCBccEJ1D"
   },
   "outputs": [],
   "source": [
    "y = df['project_is_approved'].values\n",
    "df.drop(['project_is_approved'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-KgHrZRCEJ1D"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "categorical_input = ['school_state','project_grade_category','clean_categories', 'clean_subcategories','teacher_prefix','project_is_approved']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7VJP0WAEJ1D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9sWRHI00EJ1D"
   },
   "outputs": [],
   "source": [
    "# perform stratified train test split on the dataset\n",
    "# perform stratified train test split on the dataset\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UU0xK_gYEJ1D",
    "outputId": "d3691743-8d4d-4726-a3c4-3b90ec878eef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81936,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIA3e86gEJ1E"
   },
   "source": [
    "## 1.1 Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NJ-5ZlcWEJ1E"
   },
   "outputs": [],
   "source": [
    "#since the data is already preprocessed, we can directly move to vectorization part\n",
    "#first we will vectorize the text data\n",
    "#for vectorization of text data in deep learning we use tokenizer, you can go through below references\n",
    "# https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n",
    "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "# after text vectorization you should get train_padded_docs and test_padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qWawmQUpEJ1E"
   },
   "outputs": [],
   "source": [
    "text_input = ['essay','project_title','project_resource_summary',]\n",
    "X_train['total_text_input'] = X_train['essay'] + ' ' + X_train['project_title'] + ' ' + X_train['project_resource_summary']\n",
    "X_test['total_text_input'] = X_test['essay'] + ' ' + X_test['project_title'] + ' ' + X_test['project_resource_summary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OflTZCG3EJ1E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Skkliq9NEJ1F"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 1000\n",
    "oov_token = '<UNK>'\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "# Tokenize our training data\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X_train['total_text_input'])\n",
    "\n",
    "# Get our training data word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Encode training data sentences into sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train['total_text_input'])\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test['total_text_input'])\n",
    "\n",
    "# Get max training sequence length\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "\n",
    "# Pad the training sequences\n",
    "train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n",
    "test_padded = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RShW6d93EJ1F",
    "outputId": "359ecc43-de83-41dc-f81f-0b7588882a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded training shape, Test Shape: (81936, 355) (27312, 355)\n",
      "Training sequences data type: <class 'list'> <class 'list'>\n",
      "Padded Training sequences data type: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Output the results of our work\n",
    "#print(\"Word index:\\n\", word_index)\n",
    "#print(\"\\nTraining sequences:\\n\", train_sequences)\n",
    "#print(\"\\nPadded training sequences:\\n\", train_padded)\n",
    "print(\"\\nPadded training shape, Test Shape:\", train_padded.shape,test_padded.shape)\n",
    "print(\"Training sequences data type:\", type(train_sequences),type(test_sequences))\n",
    "print(\"Padded Training sequences data type:\", type(train_padded),type(test_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uv3Fjdh0EJ1F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKrj9FriEJ1F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pbopYQJuEJ1F"
   },
   "outputs": [],
   "source": [
    "#after getting the padded_docs you have to use predefined glove vectors to get 300 dim representation for each word\n",
    "# we will be storing this data in form of an embedding matrix and will use it while defining our model\n",
    "# Please go through following blog's 'Example of Using Pre-Trained GloVe Embedding' section to understand how to create embedding matrix\n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "081h-W-QEJ1F"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suV05_PEEJ1F",
    "outputId": "aa059032-7400-45f3-f19c-398851bdb216"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:20, 19299.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "p1 = '/content/drive/MyDrive/AAIC/Assignments/LSTM on Donors Choose/glove.6B.300d.txt'\n",
    "p2 = \"C:/Users/darsh/Downloads/Srujan/Donars Choose Assignment/glove.6B.300d.txt\"\n",
    "f = open(p2,encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('\\nLoaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mN1n-ivXEJ1G"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WWYsXq0bEJ1G"
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "l9M00Zf4EJ1G"
   },
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qADF40PTEJ1G",
    "outputId": "510937ba-f1f2-4a82-c651-a929fc511eae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56772, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kb9YgIP5EJ1G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPtdVPyeEJ1G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvwkcF5JEJ1G"
   },
   "source": [
    "## 1.2 Categorical feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "vg8Q-guGEJ1G"
   },
   "outputs": [],
   "source": [
    "# for model 1 and model 2, we have to assign a unique number to each feature in a particular categorical column.\n",
    "# you can either use tokenizer,label encoder or ordinal encoder to perform the task\n",
    "# label encoder gives an error for 'unseen values' (values present in test but not in train)\n",
    "# handle unseen values with label encoder - https://stackoverflow.com/a/56876351\n",
    "# ordinal encoder also gives error with unseen values but you can use modify handle_unknown parameter\n",
    "# documentation of ordianl encoder https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
    "# after categorical feature vectorization you will have column_train_data and column_test_data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DvpV0HjmEJ1G"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "enc = OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y6oYcuvnEJ1G"
   },
   "outputs": [],
   "source": [
    "school_state_enc = (enc.fit_transform(np.array(X_train['school_state']).reshape(-1,1)))\n",
    "teacher_prefix_enc = (enc.fit_transform(np.array(X_train['teacher_prefix']).reshape(-1,1)))\n",
    "project_grade_category_enc = (enc.fit_transform(np.array(X_train['project_grade_category']).reshape(-1,1)))\n",
    "clean_categories_enc = (enc.fit_transform(np.array(X_train['clean_categories']).reshape(-1,1)))\n",
    "clean_subcategories_enc = (enc.fit_transform(np.array(X_train['clean_subcategories']).reshape(-1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IoieSEr5EJ1H"
   },
   "outputs": [],
   "source": [
    "school_state_enc_test = (enc.transform(np.array(X_test['school_state']).reshape(-1,1)))\n",
    "teacher_prefix_enc_test = (enc.transform(np.array(X_test['teacher_prefix']).reshape(-1,1)))\n",
    "project_grade_category_enc_test = (enc.transform(np.array(X_test['project_grade_category']).reshape(-1,1)))\n",
    "clean_categories_enc_test = (enc.transform(np.array(X_test['clean_categories']).reshape(-1,1)))\n",
    "clean_subcategories_enc_test = (enc.transform(np.array(X_test['clean_subcategories']).reshape(-1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54aGbVi-EJ1H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zoJP2kXEJ1H"
   },
   "source": [
    "## 1.3 Numerical feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "dJt9OBcuEJ1H"
   },
   "outputs": [],
   "source": [
    "# you have to standardise the numerical columns\n",
    "# stack both the numerical features\n",
    "#after numerical feature vectorization you will have numerical_data_train and numerical_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6XXW9NW_EJ1H"
   },
   "outputs": [],
   "source": [
    "numerical_input = ['teacher_number_of_previously_posted_projects',\n",
    "                   'resource_summary_contains_numerical_digits',\n",
    "                   'price','quantity'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ugJ01LKeEJ1H"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "scaler  = preprocessing.StandardScaler().fit(X_train[numerical_input])\n",
    "std_data_train = pd.DataFrame(scaler.transform(X_train[numerical_input]),columns=numerical_input)\n",
    "#std_data_train = ((std_data_train.astype(str).agg(','.join, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dKOLH2QQEJ1H"
   },
   "outputs": [],
   "source": [
    "std_data_test = pd.DataFrame(scaler.transform(X_test[numerical_input]),columns=numerical_input)\n",
    "#std_data_test = ((std_data_test.astype(str).agg(', '.join, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsQcF76GEJ1H",
    "outputId": "57df757f-e4b1-40c4-bbfa-e36bb7d321ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81936, 4)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(std_data_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV_DtkYuEJ1H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHGDuZYDEJ1H"
   },
   "source": [
    "## 1.4 Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-ypMubKEJ1I"
   },
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "3gn5lPDxEJ1I"
   },
   "outputs": [],
   "source": [
    "# as of now we have vectorized all our features now we will define our model.\n",
    "# as it is clear from above image that the given model has multiple input layers and hence we have to use functional API\n",
    "# Please go through - https://keras.io/guides/functional_api/\n",
    "# it is a good programming practise to define your complete model i.e all inputs , intermediate and output layers at one place.\n",
    "# while defining your model make sure that you use variable names while defining any length,dimension or size.\n",
    "#for ex.- you should write the code as 'input_text = Input(shape=(pad_length,))' and not as 'input_text = Input(shape=(300,))'\n",
    "# the embedding layer for text data should be non trainable\n",
    "# the embedding layer for categorical data should be trainable\n",
    "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "# https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0\n",
    "#print model.summary() after you have defined the model\n",
    "#plot the model using utils.plot_model module and make sure that it is similar to the above image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "plFdozCYEJ1I"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense, Input , Dropout\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ni89ZQd8EJ1I"
   },
   "outputs": [],
   "source": [
    "elements_in_school_state = (len(set(pd.DataFrame(school_state_enc)[0])))\n",
    "elements_in_teacher_prefix = (len(set(pd.DataFrame(teacher_prefix_enc)[0])))\n",
    "elements_in_project_grade_category = (len(set(pd.DataFrame(project_grade_category_enc)[0])))\n",
    "elements_in_clean_categories = (len(set(pd.DataFrame(clean_categories_enc)[0])))\n",
    "elements_in_clean_subcategories = (len(set(pd.DataFrame(clean_subcategories_enc)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LahPsXhTEJ1I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7vU50QaEJ1I",
    "outputId": "f15d2f5b-6bd7-4579-d843-a496e1c41172"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81936, 355)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FwaSV-0xEJ1I"
   },
   "outputs": [],
   "source": [
    "input_seq_total_text_data = Input(shape=(maxlen,),name='input_seq_total_text_data_')\n",
    "emb_text_data = Embedding(input_dim=vocab_size,output_dim=300, \n",
    "                          weights=[embedding_matrix], input_length=maxlen,trainable=False,\n",
    "                          name='emb_text_data')(input_seq_total_text_data)\n",
    "lstm = LSTM(units=128,activation='tanh',return_sequences=True)(emb_text_data)\n",
    "flatten_text = Flatten()(lstm)\n",
    "\n",
    "input_school_state = Input(shape=1,name='input_school_state')\n",
    "input_school_state_emb = Embedding(input_dim=elements_in_school_state,\n",
    "                                   output_dim=int(min(elements_in_school_state / 2, 50)),\n",
    "                                   input_length=1,\n",
    "                                  name='input_school_state_emb')(input_school_state)\n",
    "flatten_school_state = Flatten()(input_school_state_emb)\n",
    "\n",
    "\n",
    "input_grade_category = Input(shape=1,name='input_grade_category')\n",
    "input_grade_category_emb = Embedding(input_dim=elements_in_project_grade_category,\n",
    "                                   output_dim=int(min(elements_in_project_grade_category / 2, 50)),\n",
    "                                   input_length=1,\n",
    "                                  name='input_grade_category_emb')(input_grade_category)\n",
    "flatten_grade_category = Flatten()(input_grade_category_emb)\n",
    "\n",
    "\n",
    "input_clean_categories = Input(shape=1,name='input_clean_categories')\n",
    "input_clean_categories_emb = Embedding(input_dim=elements_in_clean_categories,\n",
    "                                      output_dim=int(min(elements_in_clean_categories / 2, 50)),\n",
    "                                      input_length=1,\n",
    "                                      name='input_clean_categories_emb')(input_clean_categories)\n",
    "flatten_clean_categories = Flatten()(input_clean_categories_emb)\n",
    "\n",
    "\n",
    "\n",
    "input_clean_sub_categories = Input(shape=1,name='input_clean_sub_categories')\n",
    "input_clean_sub_categories_emb = Embedding(input_dim=elements_in_clean_subcategories,\n",
    "                                      output_dim=int(min(elements_in_clean_subcategories / 2, 50)),\n",
    "                                      input_length=1,\n",
    "                                      name='input_clean_sub_categories_emb')(input_clean_sub_categories)\n",
    "flatten_clean_sub_categories = Flatten()(input_clean_sub_categories_emb)\n",
    "\n",
    "\n",
    "input_teacher_prefix = Input(shape=1,name='input_teacher_prefix')\n",
    "input_teacher_prefix_emb = Embedding(input_dim=elements_in_teacher_prefix,\n",
    "                                      output_dim=int(min(elements_in_teacher_prefix / 2, 50)),\n",
    "                                      input_length=1,\n",
    "                                      name='input_teacher_prefix_emb')(input_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(input_teacher_prefix_emb)\n",
    "\n",
    "\n",
    "\n",
    "input_remaining = Input(shape=4,name='input_remaining')\n",
    "input_remaining_dense = Dense(units=256,activation='relu',\n",
    "                               kernel_initializer='he_normal',kernel_regularizer=l2(0.1),\n",
    "                              name='input_remaining_dense')(input_remaining)\n",
    "flatten_remaining = Flatten()(input_remaining_dense)\n",
    "\n",
    "concat_layer = concatenate([flatten_text,flatten_school_state,flatten_grade_category,\n",
    "                            flatten_clean_categories,flatten_clean_sub_categories,\n",
    "                            flatten_teacher_prefix,flatten_remaining],)\n",
    "\n",
    "dense_layer1_after_concat = Dense(units=256,activation='relu',\n",
    "                                  kernel_initializer='he_normal',kernel_regularizer=l2(0.001),\n",
    "                                  name='dense_layer1_after_concat')(concat_layer)\n",
    "drop1 = Dropout(0.5)(dense_layer1_after_concat)\n",
    "\n",
    "dense_layer2_after_concat = Dense(units=256,activation='relu',\n",
    "                                  kernel_initializer='he_normal',kernel_regularizer=l2(0.001),\n",
    "                                  name='dense_layer2_after_concat')(drop1)\n",
    "drop2 = Dropout(0.5)(dense_layer2_after_concat)\n",
    "\n",
    "bn1 = BatchNormalization()(drop2)\n",
    "\n",
    "dense_layer3_after_concat = Dense(units=256,activation='relu',\n",
    "                                  kernel_initializer='he_normal',kernel_regularizer=l2(0.001),\n",
    "                                  name='dense_layer3_after_concat')(bn1)\n",
    "drop3 = Dropout(0.5)(dense_layer3_after_concat)\n",
    "#bn2 =  BatchNormalization()(drop3)\n",
    "\n",
    "output = Dense(units=2,activation='softmax')(drop3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qrmyxgn6EJ1J"
   },
   "outputs": [],
   "source": [
    "m1 = Model(inputs=[input_seq_total_text_data, \n",
    "                   input_school_state,\n",
    "                   input_grade_category,\n",
    "                   input_clean_categories,\n",
    "                   input_clean_sub_categories,\n",
    "                   input_teacher_prefix,\n",
    "                   input_remaining],\n",
    "           outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hREK1E_0EJ1J",
    "outputId": "1c4ed362-19ec-4981-85bd-a180b196c228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_seq_total_text_data_ (Inp [(None, 355)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb_text_data (Embedding)       (None, 355, 300)     17031600    input_seq_total_text_data_[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "input_school_state (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_grade_category (InputLaye [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_clean_categories (InputLa [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_clean_sub_categories (Inp [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_teacher_prefix (InputLaye [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_remaining (InputLayer)    [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 355, 128)     219648      emb_text_data[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_school_state_emb (Embeddi (None, 1, 25)        1275        input_school_state[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_grade_category_emb (Embed (None, 1, 2)         8           input_grade_category[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_clean_categories_emb (Emb (None, 1, 25)        1275        input_clean_categories[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_clean_sub_categories_emb  (None, 1, 50)        19650       input_clean_sub_categories[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "input_teacher_prefix_emb (Embed (None, 1, 2)         10          input_teacher_prefix[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_remaining_dense (Dense)   (None, 256)          1280        input_remaining[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 45440)        0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 25)           0           input_school_state_emb[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2)            0           input_grade_category_emb[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 25)           0           input_clean_categories_emb[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 50)           0           input_clean_sub_categories_emb[0]\n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2)            0           input_teacher_prefix_emb[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 256)          0           input_remaining_dense[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 45800)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer1_after_concat (Dens (None, 256)          11725056    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense_layer1_after_concat[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer2_after_concat (Dens (None, 256)          65792       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_layer2_after_concat[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer3_after_concat (Dens (None, 256)          65792       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_layer3_after_concat[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            514         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 29,132,924\n",
      "Trainable params: 12,100,812\n",
      "Non-trainable params: 17,032,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hkvhve-AEJ1J"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "test_data = [test_padded,school_state_enc_test,project_grade_category_enc_test,\n",
    "            clean_categories_enc_test,clean_subcategories_enc_test,teacher_prefix_enc_test,np.array(std_data_test)]\n",
    "\n",
    "train_data = [train_padded,school_state_enc,project_grade_category_enc,\n",
    "              clean_categories_enc,clean_subcategories_enc,teacher_prefix_enc,np.array(std_data_train)]             \n",
    "\n",
    "y_train_enc =  tensorflow.keras.utils.to_categorical(y_train, 2)\n",
    "y_test_enc =  tensorflow.keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4ky4ng7adjE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HM0-xUA_dmFo"
   },
   "outputs": [],
   "source": [
    "def auc1(y_true, y_pred):\n",
    "    if len(np.unique(y_true[:,1])) == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return roc_auc_score( y_true, y_pred, average='macro', sample_weight=None).astype('double')\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tensorflow.numpy_function(auc1, (y_true, y_pred), tensorflow.double)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./LSTM_Model_1.h5', save_weights_only=False,save_best_only=True, \\\n",
    "                                       mode='max', monitor='val_auroc',verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auroc', patience=2,mode='max',verbose=1),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYjXEv75Z5t0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nM0dIz4lEJ1J"
   },
   "outputs": [],
   "source": [
    "m1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WexYJGDxEJ1J",
    "outputId": "13ddd18a-5860-4d06-fa32-1312ae62a6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "641/641 [==============================] - 35s 45ms/step - loss: 28.4536 - auroc: 0.6084 - val_loss: 12.5123 - val_auroc: 0.7056\n",
      "\n",
      "Epoch 00001: val_auroc improved from -inf to 0.70555, saving model to .\\LSTM_Model_1.h5\n",
      "Epoch 2/50\n",
      "641/641 [==============================] - 28s 44ms/step - loss: 6.3930 - auroc: 0.6689 - val_loss: 2.7273 - val_auroc: 0.7120\n",
      "\n",
      "Epoch 00002: val_auroc improved from 0.70555 to 0.71196, saving model to .\\LSTM_Model_1.h5\n",
      "Epoch 3/50\n",
      "641/641 [==============================] - 28s 44ms/step - loss: 1.4311 - auroc: 0.7118 - val_loss: 0.7703 - val_auroc: 0.7289\n",
      "\n",
      "Epoch 00003: val_auroc improved from 0.71196 to 0.72890, saving model to .\\LSTM_Model_1.h5\n",
      "Epoch 4/50\n",
      "641/641 [==============================] - 28s 43ms/step - loss: 0.5491 - auroc: 0.7316 - val_loss: 0.4499 - val_auroc: 0.7381\n",
      "\n",
      "Epoch 00004: val_auroc improved from 0.72890 to 0.73811, saving model to .\\LSTM_Model_1.h5\n",
      "Epoch 5/50\n",
      "641/641 [==============================] - 29s 45ms/step - loss: 0.4298 - auroc: 0.7423 - val_loss: 0.4308 - val_auroc: 0.7468\n",
      "\n",
      "Epoch 00005: val_auroc improved from 0.73811 to 0.74678, saving model to .\\LSTM_Model_1.h5\n",
      "Epoch 6/50\n",
      "641/641 [==============================] - 29s 45ms/step - loss: 0.4161 - auroc: 0.7533 - val_loss: 0.4157 - val_auroc: 0.7494\n",
      "\n",
      "Epoch 00006: val_auroc improved from 0.74678 to 0.74943, saving model to .\\LSTM_Model_1.h5\n",
      "Epoch 7/50\n",
      "641/641 [==============================] - 29s 45ms/step - loss: 0.4074 - auroc: 0.7633 - val_loss: 0.4160 - val_auroc: 0.7485\n",
      "\n",
      "Epoch 00007: val_auroc did not improve from 0.74943\n",
      "Epoch 8/50\n",
      "641/641 [==============================] - 29s 45ms/step - loss: 0.4055 - auroc: 0.7716 - val_loss: 0.4240 - val_auroc: 0.7446\n",
      "\n",
      "Epoch 00008: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/50\n",
      "641/641 [==============================] - 29s 45ms/step - loss: 0.3679 - auroc: 0.8031 - val_loss: 0.3936 - val_auroc: 0.7461\n",
      "\n",
      "Epoch 00009: val_auroc did not improve from 0.74943\n",
      "Epoch 10/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3511 - auroc: 0.8113 - val_loss: 0.3916 - val_auroc: 0.7447\n",
      "\n",
      "Epoch 00010: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 11/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3407 - auroc: 0.8228 - val_loss: 0.3933 - val_auroc: 0.7441\n",
      "\n",
      "Epoch 00011: val_auroc did not improve from 0.74943\n",
      "Epoch 12/50\n",
      "641/641 [==============================] - 29s 45ms/step - loss: 0.3368 - auroc: 0.8246 - val_loss: 0.3934 - val_auroc: 0.7435\n",
      "\n",
      "Epoch 00012: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 13/50\n",
      "641/641 [==============================] - 29s 45ms/step - loss: 0.3334 - auroc: 0.8264 - val_loss: 0.3934 - val_auroc: 0.7434\n",
      "\n",
      "Epoch 00013: val_auroc did not improve from 0.74943\n",
      "Epoch 14/50\n",
      "641/641 [==============================] - 29s 45ms/step - loss: 0.3329 - auroc: 0.8282 - val_loss: 0.3937 - val_auroc: 0.7433\n",
      "\n",
      "Epoch 00014: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 15/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3330 - auroc: 0.8277 - val_loss: 0.3939 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00015: val_auroc did not improve from 0.74943\n",
      "Epoch 16/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3334 - auroc: 0.8274 - val_loss: 0.3935 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00016: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 17/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3337 - auroc: 0.8268 - val_loss: 0.3934 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00017: val_auroc did not improve from 0.74943\n",
      "Epoch 18/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3330 - auroc: 0.8277 - val_loss: 0.3937 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00018: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 19/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3329 - auroc: 0.8279 - val_loss: 0.3938 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00019: val_auroc did not improve from 0.74943\n",
      "Epoch 20/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3335 - auroc: 0.8271 - val_loss: 0.3940 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00020: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 21/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3333 - auroc: 0.8271 - val_loss: 0.3935 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00021: val_auroc did not improve from 0.74943\n",
      "Epoch 22/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3323 - auroc: 0.8282 - val_loss: 0.3936 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00022: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 23/50\n",
      "641/641 [==============================] - 30s 46ms/step - loss: 0.3334 - auroc: 0.8271 - val_loss: 0.3935 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00023: val_auroc did not improve from 0.74943\n",
      "Epoch 24/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3331 - auroc: 0.8272 - val_loss: 0.3936 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00024: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 25/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3330 - auroc: 0.8278 - val_loss: 0.3939 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00025: val_auroc did not improve from 0.74943\n",
      "Epoch 26/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3334 - auroc: 0.8280 - val_loss: 0.3937 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00026: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "Epoch 27/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3328 - auroc: 0.8284 - val_loss: 0.3937 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00027: val_auroc did not improve from 0.74943\n",
      "Epoch 28/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3332 - auroc: 0.8283 - val_loss: 0.3937 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00028: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "Epoch 29/50\n",
      "641/641 [==============================] - 30s 46ms/step - loss: 0.3330 - auroc: 0.8281 - val_loss: 0.3933 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00029: val_auroc did not improve from 0.74943\n",
      "Epoch 30/50\n",
      "641/641 [==============================] - 30s 46ms/step - loss: 0.3331 - auroc: 0.8278 - val_loss: 0.3936 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00030: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "Epoch 31/50\n",
      "641/641 [==============================] - 30s 46ms/step - loss: 0.3328 - auroc: 0.8280 - val_loss: 0.3940 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00031: val_auroc did not improve from 0.74943\n",
      "Epoch 32/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3324 - auroc: 0.8300 - val_loss: 0.3935 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00032: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "Epoch 33/50\n",
      "641/641 [==============================] - 30s 46ms/step - loss: 0.3331 - auroc: 0.8273 - val_loss: 0.3935 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00033: val_auroc did not improve from 0.74943\n",
      "Epoch 34/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3332 - auroc: 0.8275 - val_loss: 0.3937 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00034: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "Epoch 35/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3328 - auroc: 0.8275 - val_loss: 0.3940 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00035: val_auroc did not improve from 0.74943\n",
      "Epoch 36/50\n",
      "641/641 [==============================] - 30s 46ms/step - loss: 0.3334 - auroc: 0.8272 - val_loss: 0.3937 - val_auroc: 0.7432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "Epoch 37/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3336 - auroc: 0.8271 - val_loss: 0.3936 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00037: val_auroc did not improve from 0.74943\n",
      "Epoch 38/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3324 - auroc: 0.8287 - val_loss: 0.3935 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00038: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "Epoch 39/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3327 - auroc: 0.8282 - val_loss: 0.3937 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00039: val_auroc did not improve from 0.74943\n",
      "Epoch 40/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3330 - auroc: 0.8277 - val_loss: 0.3938 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00040: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "Epoch 41/50\n",
      "641/641 [==============================] - 30s 46ms/step - loss: 0.3325 - auroc: 0.8284 - val_loss: 0.3936 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00041: val_auroc did not improve from 0.74943\n",
      "Epoch 42/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3335 - auroc: 0.8276 - val_loss: 0.3940 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00042: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "Epoch 43/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3329 - auroc: 0.8288 - val_loss: 0.3940 - val_auroc: 0.7433\n",
      "\n",
      "Epoch 00043: val_auroc did not improve from 0.74943\n",
      "Epoch 44/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3329 - auroc: 0.8285 - val_loss: 0.3935 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00044: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "Epoch 45/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3320 - auroc: 0.8294 - val_loss: 0.3938 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00045: val_auroc did not improve from 0.74943\n",
      "Epoch 46/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3328 - auroc: 0.8271 - val_loss: 0.3932 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00046: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
      "Epoch 47/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3329 - auroc: 0.8283 - val_loss: 0.3935 - val_auroc: 0.7433\n",
      "\n",
      "Epoch 00047: val_auroc did not improve from 0.74943\n",
      "Epoch 48/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3333 - auroc: 0.8265 - val_loss: 0.3940 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00048: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
      "Epoch 49/50\n",
      "641/641 [==============================] - 29s 46ms/step - loss: 0.3333 - auroc: 0.8268 - val_loss: 0.3937 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00049: val_auroc did not improve from 0.74943\n",
      "Epoch 50/50\n",
      "641/641 [==============================] - 30s 46ms/step - loss: 0.3342 - auroc: 0.8255 - val_loss: 0.3937 - val_auroc: 0.7432\n",
      "\n",
      "Epoch 00050: val_auroc did not improve from 0.74943\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000001181490946e-25.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c763fa7700>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.fit(train_data,y_train_enc,\n",
    "       validation_data=(test_data,y_test_enc),\n",
    "       batch_size=128,\n",
    "       epochs=50,\n",
    "       callbacks=callbacks,\n",
    "       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7EShBoLUpQA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKAGNirOEJ1K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeTD-YqxEJ1N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGJXUmqBEJ1N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
