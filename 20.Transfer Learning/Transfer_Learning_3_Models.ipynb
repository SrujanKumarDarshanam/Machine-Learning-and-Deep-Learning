{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYyNlLpBpDey"
   },
   "source": [
    "#Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zrJIzFKQpBbK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "\n",
    "import os\n",
    "from keras.models import Model\n",
    "#from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qUXvy3nphQO",
    "outputId": "bcda9a86-d7aa-463c-e1a2-408d9cde0386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4442M  100 4442M    0     0   159M      0  0:00:27  0:00:27 --:--:--  158M\n"
     ]
    }
   ],
   "source": [
    "!curl --header \"Host: storage.googleapis.com\" --header \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header \"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" --header \"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/836734/1428684/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220926%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220926T163657Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=11bce130fd6090b2869efa9e76a2ef519b9987c2b56220e24ed3b48b3b77eacec62958dfbd9b13d8699a1e268bec6e66ea94c2af8eb50afa88fb49d58adb489289beace88b225dcdff62d3ea70a4b45fd28107c85034294a7d572fed5f5b4b637495498ba3c3b6463ce56ea76fd46eb62a34a915c3f0683d63b8f759c90c221c335546af8ffb654a881f8ab30dc02c8be7106f021c62b59aab83b910f4240edd070b16ada142d65e131a761b25a221488a17ac902300977272c6a0528c4f8d2b174edea844b6c691cad407f2e4a2c70d33503137648054cbf18b3c8008452ee095628befcd55e033400ece63f8cc719da73cf4a926e1f3c7088be8e1aeaf9e1d\" -L -o \"archive.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pYVZCYHphNU"
   },
   "outputs": [],
   "source": [
    "!unzip /content/archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YUSc9Q0NphDo"
   },
   "outputs": [],
   "source": [
    "traindf=pd.read_csv(\"/content/labels_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R9dWf2dTsTW1"
   },
   "outputs": [],
   "source": [
    "labels_dict={ 0 :\"letter\",\n",
    "    1 :\"form\",\n",
    "    2 :\"email\",\n",
    "    3 :\"handwritten\",\n",
    "    4 :\"advertisement\",\n",
    "    5 :\"scientific report\",\n",
    "    6 :\"scientific publication\",\n",
    "    7 :\"specification\",\n",
    "    8 :\"file folder\",\n",
    "    9 :\"news article\",\n",
    "    10 :\" budget\",\n",
    "    11 :\"invoice\",\n",
    "    12 :\" presentation\",\n",
    "    13 :\"questionnaire\",\n",
    "    14 :\"resume\",\n",
    "    15: \"memo\"}\n",
    "traindf['label']=traindf['label'].apply(lambda x:labels_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ffp1Q_K9phBX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(traindf['path'],\n",
    "                                                    traindf['label'],\n",
    "                                                    stratify = traindf['label'], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "train = pd.concat([X_train,y_train],axis=1)\n",
    "test = pd.concat([X_test,y_test],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xhW_UXfBpg-w"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255.,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGlb3advrxOk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yARISMOpg8S",
    "outputId": "4b3cdf9e-8cc3-4d3a-f878-0398632d5d98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28800 validated image filenames belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "target_size=(256,256)\n",
    "dir_path = \"/content/data_final\"\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=train,\n",
    "                                                    directory=dir_path,\n",
    "                                                    x_col=\"path\",\n",
    "                                                    y_col=\"label\",\n",
    "                                                    subset=\"training\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    #class_mode=\"categorical\",\n",
    "                                                    seed=42,shuffle=True,\n",
    "                                                    target_size = target_size,\n",
    "                                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2E6wFZ5pg5h",
    "outputId": "8288fdee-a1c3-41e7-8f35-dd6027382fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9600 validated image filenames belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = train_datagen.flow_from_dataframe(dataframe=train,\n",
    "                                              directory = dir_path,\n",
    "                                              x_col=\"path\",\n",
    "                                              y_col=\"label\",\n",
    "                                              subset=\"validation\",\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode=\"categorical\",\n",
    "                                              seed=42,shuffle=True,\n",
    "                                              target_size=target_size,                                              \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BuauZ0wpg1S",
    "outputId": "8a485f66-8096-417f-8520-d71f044dfe2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9600 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe=test,\n",
    "                                                  directory = dir_path,\n",
    "                                                  x_col=\"path\",\n",
    "                                                  y_col=None,\n",
    "                                                  class_mode=None,\n",
    "                                                  seed=42,shuffle=False,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  target_size=target_size\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XToqob_pgyl"
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "    conv_base = VGG16(include_top=False,\n",
    "                      weights='imagenet', \n",
    "                      input_shape=input_shape)    \n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    top_model = conv_base.output\n",
    "    \n",
    "    #Conv1 = Conv2D(filters=256,kernel_size=(7,7),strides=(1, 1),padding=\"valid\",data_format='channels_last',activation='relu',kernel_initializer=tf.keras.initializers.he_normal(seed=0))(top_model)\n",
    "    \n",
    "    Conv1 = Conv2D(filters=16,kernel_size=(3,3),padding='same',activation='relu',)(top_model)\n",
    "    #MaxPool Layer\n",
    "    Pool1 = MaxPool2D(pool_size=(2,2))(Conv1)\n",
    "\n",
    "    flatten = Flatten()(Pool1)\n",
    "\n",
    "    #FC layer\n",
    "    FC1 = Dense(units=512,activation='relu')(flatten)\n",
    "    FC1 = (Dropout(0.5))(FC1)\n",
    "    #FC layer\n",
    "    FC2 = Dense(units=64,activation='relu')(FC1)\n",
    "\n",
    "    #output layer\n",
    "    Out = Dense(units=16,activation='softmax')(FC2)\n",
    "    model = Model(inputs=conv_base.input,outputs=Out)\n",
    "\n",
    "    # Compiles the model for training.\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pEg_u1zpgwF",
    "outputId": "7b9be6b9-05ba-4c97-e0a5-dd5aae3de31c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.0001,decay=1e-6)\n",
    "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "#session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options)).close()\n",
    "BATCH_SIZE = batch_size\n",
    "input_shape = (256,256,3)\n",
    "optim_1 =  RMSprop #'rmsprop'#Adam(learning_rate=0.1)\n",
    "n_classes=16\n",
    "\n",
    "#n_steps = train_generator.n // BATCH_SIZE\n",
    "#n_val_steps = test_generator.n // BATCH_SIZE\n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "n_epochs = 300\n",
    "\n",
    "# First we'll train the model without Fine-tuning\n",
    "vgg_model_1 = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hinLA3Cmpgtj",
    "outputId": "57e6f820-6c35-4f85-f127-41e065f8b694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 8, 8, 16)          73744     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 4, 4, 16)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,953,888\n",
      "Trainable params: 239,200\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37PJchcjpgqx"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "filepath = \"Model_1_Transfer_Learning_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "history = tf.keras.callbacks.History()\n",
    "\n",
    "# tensorboard\n",
    "tensorboard = TensorBoard(log_dir=\"model_logs/{}\".format(time()))\n",
    "\n",
    "filepath = \"Model_1_Transfer_Learning_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.001)\n",
    "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "sch = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "#from livelossplot.inputs.keras import PlotLossesCallback\n",
    "\n",
    "#plot_loss_1 = PlotLossesCallback()\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint_save,learning_rate_reduction,history,tensorboard,]#,sch,plot_loss_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_ZhH5wgpgoA",
    "outputId": "375d390a-ae7d-440a-918a-9e164c16fce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 2.1003 - accuracy: 0.3347\n",
      "Epoch 1: val_accuracy improved from -inf to 0.49542, saving model to Model_1_Transfer_Learning_weights.01-1.59.hdf5\n",
      "900/900 [==============================] - 266s 278ms/step - loss: 2.1003 - accuracy: 0.3347 - val_loss: 1.5936 - val_accuracy: 0.4954 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 1.5197 - accuracy: 0.5285\n",
      "Epoch 2: val_accuracy improved from 0.49542 to 0.59281, saving model to Model_1_Transfer_Learning_weights.02-1.33.hdf5\n",
      "900/900 [==============================] - 241s 268ms/step - loss: 1.5197 - accuracy: 0.5285 - val_loss: 1.3279 - val_accuracy: 0.5928 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 1.3142 - accuracy: 0.5992\n",
      "Epoch 3: val_accuracy improved from 0.59281 to 0.64323, saving model to Model_1_Transfer_Learning_weights.03-1.19.hdf5\n",
      "900/900 [==============================] - 239s 265ms/step - loss: 1.3142 - accuracy: 0.5992 - val_loss: 1.1933 - val_accuracy: 0.6432 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 1.2083 - accuracy: 0.6333\n",
      "Epoch 4: val_accuracy improved from 0.64323 to 0.65948, saving model to Model_1_Transfer_Learning_weights.04-1.12.hdf5\n",
      "900/900 [==============================] - 236s 263ms/step - loss: 1.2083 - accuracy: 0.6333 - val_loss: 1.1203 - val_accuracy: 0.6595 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 1.1289 - accuracy: 0.6566\n",
      "Epoch 5: val_accuracy improved from 0.65948 to 0.67302, saving model to Model_1_Transfer_Learning_weights.05-1.07.hdf5\n",
      "900/900 [==============================] - 236s 262ms/step - loss: 1.1289 - accuracy: 0.6566 - val_loss: 1.0734 - val_accuracy: 0.6730 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 1.0616 - accuracy: 0.6768\n",
      "Epoch 6: val_accuracy improved from 0.67302 to 0.68313, saving model to Model_1_Transfer_Learning_weights.06-1.03.hdf5\n",
      "900/900 [==============================] - 236s 262ms/step - loss: 1.0616 - accuracy: 0.6768 - val_loss: 1.0339 - val_accuracy: 0.6831 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 1.0076 - accuracy: 0.6919\n",
      "Epoch 7: val_accuracy improved from 0.68313 to 0.69583, saving model to Model_1_Transfer_Learning_weights.07-1.01.hdf5\n",
      "900/900 [==============================] - 236s 262ms/step - loss: 1.0076 - accuracy: 0.6919 - val_loss: 1.0071 - val_accuracy: 0.6958 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.9763 - accuracy: 0.7025\n",
      "Epoch 8: val_accuracy improved from 0.69583 to 0.70625, saving model to Model_1_Transfer_Learning_weights.08-0.97.hdf5\n",
      "900/900 [==============================] - 236s 262ms/step - loss: 0.9763 - accuracy: 0.7025 - val_loss: 0.9663 - val_accuracy: 0.7063 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.9328 - accuracy: 0.7180\n",
      "Epoch 9: val_accuracy improved from 0.70625 to 0.71667, saving model to Model_1_Transfer_Learning_weights.09-0.94.hdf5\n",
      "900/900 [==============================] - 236s 262ms/step - loss: 0.9328 - accuracy: 0.7180 - val_loss: 0.9400 - val_accuracy: 0.7167 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.9023 - accuracy: 0.7249\n",
      "Epoch 10: val_accuracy did not improve from 0.71667\n",
      "900/900 [==============================] - 236s 262ms/step - loss: 0.9023 - accuracy: 0.7249 - val_loss: 0.9393 - val_accuracy: 0.7151 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "vgg_history = vgg_model_1.fit(train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=10,callbacks=callbacks_list,verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91AL9bv0rKp4"
   },
   "source": [
    "##Model-1 Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZi9v-6NpglP"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "best_model_path = \"/content/Model_1_Transfer_Learning_weights.09-0.94.hdf5\"\n",
    "model = keras.models.load_model(best_model_path)\n",
    "#model = vgg_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NzmCcdepBX3",
    "outputId": "c04585d1-9d6e-4881-cad5-498d89dd8ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 58s 192ms/step - loss: 0.9400 - accuracy: 0.7167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9399949908256531, 0.7166666388511658]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "model.evaluate(valid_generator,steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itWqLXaWpBPf",
    "outputId": "ea281db4-63bf-429e-a8ab-bea924309928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 69s 230ms/step\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQDloo-YrVso"
   },
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ES5o3LLrVqJ"
   },
   "outputs": [],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA0BatLXrVnl"
   },
   "outputs": [],
   "source": [
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MQaXnwbrVlV",
    "outputId": "d7b3bcc6-645f-4e9e-b91b-02297ffa8e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Model Accuracy without Fine-Tuning: 70.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vgg_acc = accuracy_score(test['label'], predictions)\n",
    "print(\"VGG16 Model Accuracy without Fine-Tuning: {:.2f}%\".format(vgg_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nJh9UF6rcq7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De0UlsaOcAE1"
   },
   "source": [
    "#Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNXN3EXFcAE5"
   },
   "source": [
    "<pre>\n",
    "1. Use <a href='https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16'>VGG-16</a> pretrained network without Fully Connected layers and initilize all the weights with Imagenet trained weights.\n",
    "2. After VGG-16 network without FC layers, don't use FC layers, use conv layers only as Fully connected layer. any FC layer can be converted to a CONV layer. This conversion will reduce the No of Trainable parameters in FC layers. For example, an FC layer with K=4096 that is looking at some input volume of size 7×7×512 can be equivalently expressed as a CONV layer with F=7,P=0,S=1,K=4096. In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be 1×1×4096 since only a single depth column “fits” across the input volume, giving identical result as the initial FC layer. You can refer <a href='http://cs231n.github.io/convolutional-networks/#convert'>this</a> link to better understanding of using Conv layer in place of fully connected layers.\n",
    "3. Final architecture will be VGG-16 without FC layers(without top), 2 Conv layers identical to FC layers, 1 output layer for 16 class classification. <b>INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer</b>\n",
    "3. Train only last 2 Conv layers identical to FC layers, 1 output layer. Don't train the VGG-16 network. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qI2b2qT-PrKw"
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "    conv_base = VGG16(include_top=False,\n",
    "                      weights='imagenet', \n",
    "                      input_shape=input_shape)    \n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    top_model = conv_base.output\n",
    "    #F=7,P=0,S=1,K=4096 \n",
    "    #top_model = Conv2D(filters=16,kernel_size=(3,3),padding=\"valid\",data_format='channels_last',activation='relu',kernel_initializer=tf.keras.initializers.he_normal,kernel_regularizer=regularizers.l2(0.0001),)(top_model)\n",
    "    #top_model = Conv2D(filters=8,kernel_size=(3,3),padding='valid',data_format='channels_last',\n",
    "    #          activation='relu',kernel_initializer=tf.keras.initializers.he_normal,)(top_model)\n",
    "    #top_model = Conv2D(filters=8,kernel_size=(3,3),padding='valid',data_format='channels_last',\n",
    "    #          activation='relu',kernel_initializer=tf.keras.initializers.he_normal,)(top_model)\n",
    "    #top_model = Conv2D(filters=16,kernel_size=(3,3),padding=\"valid\",data_format='channels_last',activation='relu',kernel_initializer=tf.keras.initializers.he_normal,kernel_regularizer=regularizers.l2(0.0001),)(top_model)\n",
    "    top_model = Conv2D(filters=4096,kernel_size=(8,8) ,strides=(1,1),activation=\"relu\")(top_model)\n",
    "    top_model = Conv2D(filters=4096,kernel_size=(1,1) ,strides=(1,1),activation=\"relu\")(top_model)\n",
    "    top_model = Flatten()(top_model)\n",
    "    output_layer = Dense(units=n_classes,activation='softmax',)(top_model)\n",
    "    #output_layer = Conv2D(filters=16,kernel_size=(1,1),strides=(1, 1),padding=\"valid\")(top_model)\n",
    "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "\n",
    "    # Compiles the model for training.\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBHDE8eQiRYG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-jozXjzPrJg",
    "outputId": "7466350f-dd00-441d-c9fa-0e074e81ff02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from keras import regularizers\n",
    "RMSprop = tf.keras.optimizers.RMSprop()\n",
    "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "#session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options)).close()\n",
    "BATCH_SIZE = batch_size\n",
    "input_shape = (256,256,3)\n",
    "optim_1 =  \"Adam\"#RMSprop #'rmsprop'#Adam(learning_rate=0.1)\n",
    "n_classes=16\n",
    "\n",
    "#n_steps = train_generator.samples // BATCH_SIZE\n",
    "#n_val_steps = valid_generator.samples // BATCH_SIZE\n",
    "#n_epochs = 300\n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "n_epochs = 300\n",
    "\n",
    "# First we'll train the model without Fine-tuning\n",
    "vgg_model_2 = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZkQ9dMEPKpr",
    "outputId": "3f053237-4460-41a7-a3d5-9f020f47d51a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 1, 1, 4096)        134221824 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                65552     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165,783,376\n",
      "Trainable params: 151,068,688\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFopsSp0SlZ5"
   },
   "outputs": [],
   "source": [
    "#pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zp5YKkkPrFZ"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "filepath = \"Model_2_TL_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "history = tf.keras.callbacks.History()\n",
    "\n",
    "# tensorboard\n",
    "tensorboard = TensorBoard(log_dir=\"model_logs/{}\".format(time()))\n",
    "\n",
    "filepath = \"Model_2_TL_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.001)\n",
    "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "sch = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "#from livelossplot.inputs.keras import PlotLossesCallback\n",
    "\n",
    "#plot_loss_1 = PlotLossesCallback()\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint_save,learning_rate_reduction,history,tensorboard,]#,sch,plot_loss_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "HRz-3bMuPrBi",
    "outputId": "839a5f4d-fafd-4bf3-acda-1dce6317a1bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "900/900 [==============================] - ETA: 0s - loss: 1.5361 - accuracy: 0.5861\n",
      "Epoch 1: val_accuracy improved from -inf to 0.64198, saving model to Model_2_TL_weights.01-1.18.hdf5\n",
      "900/900 [==============================] - 437s 469ms/step - loss: 1.5361 - accuracy: 0.5861 - val_loss: 1.1835 - val_accuracy: 0.6420 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.9342 - accuracy: 0.7152\n",
      "Epoch 2: val_accuracy improved from 0.64198 to 0.72573, saving model to Model_2_TL_weights.02-0.93.hdf5\n",
      "900/900 [==============================] - 421s 468ms/step - loss: 0.9342 - accuracy: 0.7152 - val_loss: 0.9289 - val_accuracy: 0.7257 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.8032 - accuracy: 0.7529\n",
      "Epoch 3: val_accuracy did not improve from 0.72573\n",
      "900/900 [==============================] - 413s 459ms/step - loss: 0.8032 - accuracy: 0.7529 - val_loss: 0.9257 - val_accuracy: 0.7233 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.7910\n",
      "Epoch 4: val_accuracy did not improve from 0.72573\n",
      "900/900 [==============================] - 413s 458ms/step - loss: 0.6803 - accuracy: 0.7910 - val_loss: 0.9456 - val_accuracy: 0.7226 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.5943 - accuracy: 0.8158\n",
      "Epoch 5: val_accuracy improved from 0.72573 to 0.72771, saving model to Model_2_TL_weights.05-0.98.hdf5\n",
      "900/900 [==============================] - 421s 468ms/step - loss: 0.5943 - accuracy: 0.8158 - val_loss: 0.9796 - val_accuracy: 0.7277 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.5290 - accuracy: 0.8365\n",
      "Epoch 6: val_accuracy did not improve from 0.72771\n",
      "900/900 [==============================] - 413s 459ms/step - loss: 0.5290 - accuracy: 0.8365 - val_loss: 1.0620 - val_accuracy: 0.7056 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.4829 - accuracy: 0.8492\n",
      "Epoch 7: val_accuracy improved from 0.72771 to 0.73125, saving model to Model_2_TL_weights.07-1.01.hdf5\n",
      "900/900 [==============================] - 422s 469ms/step - loss: 0.4829 - accuracy: 0.8492 - val_loss: 1.0149 - val_accuracy: 0.7312 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.4485 - accuracy: 0.8589\n",
      "Epoch 8: val_accuracy did not improve from 0.73125\n",
      "900/900 [==============================] - 413s 459ms/step - loss: 0.4485 - accuracy: 0.8589 - val_loss: 1.1397 - val_accuracy: 0.7159 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "113/900 [==>...........................] - ETA: 4:53 - loss: 0.3720 - accuracy: 0.8855"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-56df173127c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg_history = vgg_model_2.fit(train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=20,callbacks=callbacks_list,verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbM1TYLhPq8D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbymLvyQPKps"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1vh-PEoC3Gb"
   },
   "source": [
    "##Model-2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3svesjkyPq4T"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "best_model_path = \"/content/Model_2_TL_weights.07-1.01.hdf5\"\n",
    "model = keras.models.load_model(best_model_path)\n",
    "#model = vgg_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dx78dM0b1SW8",
    "outputId": "c650b0a2-b5b2-4bd3-8334-80c727b8e80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 80s 264ms/step - loss: 1.0149 - accuracy: 0.7312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0148907899856567, 0.731249988079071]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "model.evaluate(valid_generator,steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyssiDk21SQA",
    "outputId": "07f7444d-2a5d-4e69-bf42-55af1ce0ba58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 77s 256ms/step\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Rg96WXY1SI-"
   },
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7sIlgCYPqul"
   },
   "outputs": [],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_N6HLghmPKpt"
   },
   "outputs": [],
   "source": [
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS2_mAwsPKpt",
    "outputId": "b12585b2-ad72-4d85-a2ae-f1d6be36dabb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Model Accuracy without Fine-Tuning: 72.34%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vgg_acc = accuracy_score(test['label'], predictions)\n",
    "print(\"VGG16 Model Accuracy without Fine-Tuning: {:.2f}%\".format(vgg_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjJFhswlPKpu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOVFG14_pvzu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFXeQqZbpUB7"
   },
   "source": [
    "#Model-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g47aqYERu1O"
   },
   "source": [
    "<pre>\n",
    "1. Use same network as Model-2 '<b>INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer</b>' and train only Last 6 Layers of VGG-16 network, 2 Conv layers identical to FC layers, 1 output layer.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5Lbq85GOpdXh"
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "    conv_base = VGG16(include_top=False,\n",
    "                      weights='imagenet', \n",
    "                      input_shape=input_shape)    \n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    top_model = conv_base.output\n",
    "    top_model = Conv2D(filters=4096,kernel_size=(8,8) ,strides=(1,1),activation=\"relu\")(top_model)\n",
    "    top_model = Conv2D(filters=4096,kernel_size=(1,1) ,strides=(1,1),activation=\"relu\")(top_model)\n",
    "    top_model = Flatten()(top_model)\n",
    "    #output layer\n",
    "    Out = Dense(units=16,activation='softmax')(top_model)\n",
    "    model = Model(inputs=conv_base.input,outputs=Out)\n",
    "\n",
    "    # Compiles the model for training.\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cB5tSTUwpdUt",
    "outputId": "e4e27f1e-c67b-4cfd-8408-a374314e46f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.0001,rho=0.9,momentum=0,epsilon=1e-07,centered=False,name=\"RMSprop\")\n",
    "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "#session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options)).close()\n",
    "BATCH_SIZE = batch_size\n",
    "input_shape = (256,256,3)\n",
    "optim_1 =  Adam(learning_rate=0.1)\n",
    "n_classes=16\n",
    "\n",
    "#n_steps = train_generator.samples // BATCH_SIZE\n",
    "#n_val_steps = valid_generator.samples // BATCH_SIZE\n",
    "#n_epochs = 300\n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "n_epochs = 300\n",
    "\n",
    "# First we'll train the model without Fine-tuning\n",
    "vgg_model_3 = create_model(input_shape, n_classes, optim_1, fine_tune=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzmoIjlNpdR7",
    "outputId": "56a3c4c3-c5a5-46cf-aa15-91faaa16ae12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 1, 1, 4096)        134221824 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                65552     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165,783,376\n",
      "Trainable params: 160,507,920\n",
      "Non-trainable params: 5,275,456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aZjtmQTHpdKu"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "filepath = \"Model_3_TL_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "history = tf.keras.callbacks.History()\n",
    "\n",
    "# tensorboard\n",
    "tensorboard = TensorBoard(log_dir=\"model_logs/{}\".format(time()))\n",
    "\n",
    "filepath = \"Model_3_TL_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.001)\n",
    "checkpoint_save = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "sch = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "#from livelossplot.inputs.keras import PlotLossesCallback\n",
    "\n",
    "#plot_loss_1 = PlotLossesCallback()\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint_save,learning_rate_reduction,history,tensorboard,]#,sch,plot_loss_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62vHFhIESmy9",
    "outputId": "c132e3bc-e301-4e11-8497-6d255b1996cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 198554681344.0000 - accuracy: 0.0614\n",
      "Epoch 1: val_accuracy improved from -inf to 0.06458, saving model to Model_3_TL_weights.01-2362942.50.hdf5\n",
      "900/900 [==============================] - 670s 725ms/step - loss: 198554681344.0000 - accuracy: 0.0614 - val_loss: 2362942.5000 - val_accuracy: 0.0646 - lr: 0.1000\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 12616.7188 - accuracy: 0.0634\n",
      "Epoch 2: val_accuracy did not improve from 0.06458\n",
      "900/900 [==============================] - 646s 717ms/step - loss: 12616.7188 - accuracy: 0.0634 - val_loss: 690270.9375 - val_accuracy: 0.0616 - lr: 0.1000\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 184150448.0000 - accuracy: 0.0593\n",
      "Epoch 3: val_accuracy did not improve from 0.06458\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "900/900 [==============================] - 654s 727ms/step - loss: 184150448.0000 - accuracy: 0.0593 - val_loss: 3.1556 - val_accuracy: 0.0624 - lr: 0.1000\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 2.8172 - accuracy: 0.0609\n",
      "Epoch 4: val_accuracy did not improve from 0.06458\n",
      "900/900 [==============================] - 646s 717ms/step - loss: 2.8172 - accuracy: 0.0609 - val_loss: 2.7740 - val_accuracy: 0.0597 - lr: 0.0500\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 2.7777 - accuracy: 0.0603\n",
      "Epoch 5: val_accuracy did not improve from 0.06458\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "900/900 [==============================] - 646s 718ms/step - loss: 2.7777 - accuracy: 0.0603 - val_loss: 2.7812 - val_accuracy: 0.0616 - lr: 0.0500\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 2.7756 - accuracy: 0.0655\n",
      "Epoch 6: val_accuracy did not improve from 0.06458\n",
      "900/900 [==============================] - 646s 717ms/step - loss: 2.7756 - accuracy: 0.0655 - val_loss: 2.7750 - val_accuracy: 0.0599 - lr: 0.0250\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 2.7764 - accuracy: 0.0619\n",
      "Epoch 7: val_accuracy did not improve from 0.06458\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "900/900 [==============================] - 646s 717ms/step - loss: 2.7764 - accuracy: 0.0619 - val_loss: 2.7762 - val_accuracy: 0.0636 - lr: 0.0250\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 2.7745 - accuracy: 0.0607\n",
      "Epoch 8: val_accuracy did not improve from 0.06458\n",
      "900/900 [==============================] - 646s 717ms/step - loss: 2.7745 - accuracy: 0.0607 - val_loss: 2.7742 - val_accuracy: 0.0605 - lr: 0.0125\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 2.7746 - accuracy: 0.0622\n",
      "Epoch 9: val_accuracy did not improve from 0.06458\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "900/900 [==============================] - 645s 717ms/step - loss: 2.7746 - accuracy: 0.0622 - val_loss: 2.7745 - val_accuracy: 0.0595 - lr: 0.0125\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - ETA: 0s - loss: 2.7737 - accuracy: 0.0618\n",
      "Epoch 10: val_accuracy did not improve from 0.06458\n",
      "900/900 [==============================] - 645s 717ms/step - loss: 2.7737 - accuracy: 0.0618 - val_loss: 2.7741 - val_accuracy: 0.0599 - lr: 0.0063\n"
     ]
    }
   ],
   "source": [
    "vgg_history = vgg_model_3.fit(train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=10,callbacks=callbacks_list,verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgRCPJyLSmun"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L6mcyWqCpXI"
   },
   "source": [
    "##Model-3 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TpTFkpvcSmr_"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "best_model_path = \"/content/Model_3_TL_weights.01-2362942.50.hdf5\"\n",
    "model = keras.models.load_model(best_model_path)\n",
    "#model = vgg_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1Xecxj6Smpp",
    "outputId": "8cd5862f-9b01-4f7d-ecde-f0fdd82f22d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 84s 281ms/step - loss: 2.7741 - accuracy: 0.0599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.774121046066284, 0.0598958320915699]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vgg_model_3\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "model.evaluate(valid_generator,steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4kNk8oqtSmnW",
    "outputId": "89c7f7d2-8b9e-4d90-90eb-858c62bd39f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 82s 271ms/step\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Ip9wYB26Smke"
   },
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4enCAFWTSmiC"
   },
   "outputs": [],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SZGeqMSJSmfe"
   },
   "outputs": [],
   "source": [
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1nuxaUDSmct",
    "outputId": "b7466e96-18c4-4d57-db82-a19fe2826f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Model Accuracy without Fine-Tuning: 6.26%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vgg_acc = accuracy_score(test['label'], predictions)\n",
    "print(\"VGG16 Model Accuracy without Fine-Tuning: {:.2f}%\".format(vgg_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY8riT-6SmZ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCjunjeISmXf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxRN2O7YEIFE"
   },
   "source": [
    "#Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlWQoQybSmU_",
    "outputId": "72843d61-d635-42e5-e64e-c5dc516fe2e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+---------------+\n",
      "| Sl.N0 |  Model  | Number of epochs |  val_accuracy |\n",
      "+-------+---------+------------------+---------------+\n",
      "|   1   | Model-1 |        10        |     0.7089    |\n",
      "+-------+---------+------------------+---------------+\n",
      "|   2   | Model-2 |        9         |     0.7234    |\n",
      "+-------+---------+------------------+---------------+\n",
      "|   3   | Model-3 |        10        |     0.0626    |\n",
      "+-------+---------+------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "from prettytable import ALL as ALL\n",
    "table=PrettyTable(hrules=ALL)\n",
    "table.field_names = [ \"Sl.N0\", \"Model\", \"Number of epochs\", \" val_accuracy\"] # # http://zetcode.com/python/prettytable/\n",
    "table.add_row([1, \"Model-1\", \"10\", 70.89/100])\n",
    "table.add_row([2, \"Model-2\",\"9\", 72.34/100])\n",
    "table.add_row([3, \"Model-3\",\"10\", 6.26/100])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL6nlnNNrNF8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0DcQX3OfxPE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNBxOacEfxI1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
