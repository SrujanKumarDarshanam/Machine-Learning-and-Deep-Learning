{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use DropOut layers.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3757,
     "status": "ok",
     "timestamp": 1660644217410,
     "user": {
      "displayName": "Srujan kumar",
      "userId": "03597094715646497171"
     },
     "user_tz": -330
    },
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import A`dam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.data import chelsea,astronaut\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1660644219665,
     "user": {
      "displayName": "Srujan kumar",
      "userId": "03597094715646497171"
     },
     "user_tz": -330
    },
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1660644221506,
     "user": {
      "displayName": "Srujan kumar",
      "userId": "03597094715646497171"
     },
     "user_tz": -330
    },
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "num_filter = 12\n",
    "compression = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6007,
     "status": "ok",
     "timestamp": 1660644229404,
     "user": {
      "displayName": "Srujan kumar",
      "userId": "03597094715646497171"
     },
     "user_tz": -330
    },
    "id": "mB7o3zu1g6eT",
    "outputId": "a1f47a1e-c897-4d2c-ad7b-659dbc378cde"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_train, X_cv, y_train, y_cv = train_test_split(x_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert to one hot encoing \n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, num_classes) \n",
    "#y_cv = tf.keras.utils.to_categorical(y_cv, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1660624420132,
     "user": {
      "displayName": "Srujan kumar",
      "userId": "03597094715646497171"
     },
     "user_tz": -330
    },
    "id": "3lAk_Mw_5-rn",
    "outputId": "7548d2f2-cea2-4eac-cc83-d1dcf6bc1113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), 50000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1660624423075,
     "user": {
      "displayName": "Srujan kumar",
      "userId": "03597094715646497171"
     },
     "user_tz": -330
    },
    "id": "DVkpgHsc5-rp",
    "outputId": "a5c4bf5e-f987-473b-c0e3-2e26345a32d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_height, img_width, channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "from tensorflow.keras import regularizers\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same',\n",
    "                                   kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                                   kernel_regularizer=regularizers.L2(0.0001)\n",
    "                                  )(relu)\n",
    "        #if dropout_rate>0:\n",
    "        #    Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same',\n",
    "                                     kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                                      kernel_regularizer=regularizers.L2(0.0001)\n",
    "                                     )(relu)\n",
    "    #if dropout_rate>0:\n",
    "    #     Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2),)(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "num_filter = 80\n",
    "dropout_rate = 0.0\n",
    "l = 12\n",
    "\n",
    "from tensorflow.keras.layers import DepthwiseConv2D,SeparableConv2D,Conv2DTranspose,Conv3D,Conv3DTranspose\n",
    "\n",
    "\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3),strides=(1,1), use_bias=False ,padding='same',activation = 'relu',\n",
    "                             kernel_initializer=tf.keras.initializers.HeNormal(),kernel_regularizer=regularizers.L2(0.0001)\n",
    "                            )(input)\n",
    "\n",
    "#First_Conv2D = DepthwiseConv2D(kernel_size = (3,3), use_bias=False ,padding='same',activation = 'relu',\n",
    "#                             depthwise_initializer=tf.keras.initializers.HeUniform(),\n",
    "#                             depthwise_regularizer=regularizers.L2())(input)\n",
    "\n",
    "#First_Conv2D = Conv2DTranspose(kernel_size = (10,10), filters = num_filter,activation = 'relu',\n",
    "#                               use_bias=False ,padding='same',\n",
    "#                               kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "#                               kernel_regularizer=regularizers.L2())(input)\n",
    "\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 32, 32, 560]),\n",
       " TensorShape([None, 16, 16, 520]),\n",
       " TensorShape([None, 8, 8, 520]),\n",
       " TensorShape([None, 4, 4, 520]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "First_Block.shape,Second_Block.shape,Third_Block.shape,Last_Block.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1660375723236,
     "user": {
      "displayName": "Srujan kumar",
      "userId": "03597094715646497171"
     },
     "user_tz": -330
    },
    "id": "c5Wksy8z5-rw",
    "outputId": "f2082c36-96fe-46f7-e973-7e3fc1ce3222"
   },
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1608.06993.pdf\n",
    "#from IPython.display import IFrame, YouTubeVideo\n",
    "#YouTubeVideo(id='-W6y8xnd--U', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1kFh7pdxhNtT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 80)   2160        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 80)   320         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 80)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 40)   28800       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 120)  0           conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 120)  480         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 120)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 40)   43200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 160)  0           concatenate[0][0]                \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 40)   57600       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 200)  0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 200)  800         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 200)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 40)   72000       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 240)  0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 240)  960         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 240)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 40)   86400       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 280)  0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 280)  1120        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 280)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 40)   100800      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 320)  0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 320)  1280        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 320)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 40)   115200      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 360)  0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 360)  1440        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 360)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 40)   129600      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 400)  0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 400)  1600        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 400)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 40)   144000      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 440)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 440)  1760        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 440)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 40)   158400      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 480)  0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 480)  1920        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 480)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 40)   172800      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 520)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 520)  2080        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 520)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 40)   187200      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 560)  0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 560)  2240        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 560)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 40)   22400       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 40)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 40)   160         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 40)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 40)   14400       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 80)   0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 80)   320         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 80)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 40)   28800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 120)  0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 120)  480         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 120)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 40)   43200       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 160)  0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 160)  640         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 160)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 40)   57600       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 200)  0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 200)  800         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 200)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 40)   72000       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 240)  0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 240)  960         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 240)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 40)   86400       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 280)  0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 280)  1120        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 280)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 40)   100800      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 320)  0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 320)  1280        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 320)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 40)   115200      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 360)  0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 360)  1440        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 360)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 40)   129600      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 400)  0           concatenate_19[0][0]             \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 400)  1600        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 400)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 40)   144000      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 440)  0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 440)  1760        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 440)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 40)   158400      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 480)  0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 480)  1920        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 480)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 40)   172800      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 520)  0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 520)  2080        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 520)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 40)   20800       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 40)     0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 40)     160         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 40)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 40)     14400       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 80)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 80)     320         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 80)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 40)     28800       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 120)    0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 120)    480         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 120)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 40)     43200       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 160)    0           concatenate_25[0][0]             \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 160)    640         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 160)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 40)     57600       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 200)    0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 200)    800         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 200)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 40)     72000       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 240)    0           concatenate_27[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 240)    960         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 240)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 40)     86400       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 280)    0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 280)    1120        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 280)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 40)     100800      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 320)    0           concatenate_29[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 320)    1280        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 320)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 40)     115200      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 8, 8, 360)    0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 360)    1440        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 360)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 40)     129600      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 400)    0           concatenate_31[0][0]             \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 400)    1600        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 400)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 40)     144000      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 440)    0           concatenate_32[0][0]             \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 440)    1760        concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 440)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 40)     158400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 480)    0           concatenate_33[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 480)    1920        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 480)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 40)     172800      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 520)    0           concatenate_34[0][0]             \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 520)    2080        concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 520)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 40)     20800       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 40)     0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 4, 4, 40)     160         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 40)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 4, 4, 40)     14400       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 4, 4, 80)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 80)     320         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 80)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 40)     28800       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 4, 4, 120)    0           concatenate_36[0][0]             \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 4, 4, 120)    480         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 120)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 40)     43200       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 4, 4, 160)    0           concatenate_37[0][0]             \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 4, 4, 160)    640         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 160)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 40)     57600       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 4, 4, 200)    0           concatenate_38[0][0]             \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 200)    800         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 200)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 40)     72000       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 4, 4, 240)    0           concatenate_39[0][0]             \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 240)    960         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 240)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 40)     86400       activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 4, 4, 280)    0           concatenate_40[0][0]             \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 280)    1120        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 280)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 40)     100800      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 4, 4, 320)    0           concatenate_41[0][0]             \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 320)    1280        concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 320)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 40)     115200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 4, 4, 360)    0           concatenate_42[0][0]             \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 360)    1440        concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 360)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 40)     129600      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 4, 4, 400)    0           concatenate_43[0][0]             \n",
      "                                                                 conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 400)    1600        concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 400)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 40)     144000      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 4, 4, 440)    0           concatenate_44[0][0]             \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 440)    1760        concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 440)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 40)     158400      activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 4, 4, 480)    0           concatenate_45[0][0]             \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 480)    1920        concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 480)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 40)     172800      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 4, 4, 520)    0           concatenate_46[0][0]             \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 520)    2080        concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 520)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 2, 2, 520)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2080)         0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           20810       flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,812,890\n",
      "Trainable params: 4,782,730\n",
      "Non-trainable params: 30,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1660624441785,
     "user": {
      "displayName": "Srujan kumar",
      "userId": "03597094715646497171"
     },
     "user_tz": -330
    },
    "id": "8Aqzk9AFXb1y",
    "outputId": "74829a4c-c531-4860-8c60-79f7b96e9bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 128\n",
    "val_batch_size = 128\n",
    "steps = len(Y_train)//batch_size\n",
    "val_steps = len(Y_test)//val_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "      width_shift_range = 0.1,height_shift_range = 0.1,#rescale=1./255.,\n",
    "    horizontal_flip = True,rotation_range = 10,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    zoom_range = 0.2, shear_range = 10,\n",
    ")\n",
    "train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "      width_shift_range = 0.1,height_shift_range = 0.1,#rescale=1./255.,\n",
    "    horizontal_flip = True,rotation_range = 10,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    zoom_range = 0.2, shear_range = 10,\n",
    ")\n",
    "\n",
    "test_datagen.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.data import chelsea,astronaut\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "imgs = np.stack([X_train[1111] for i in range(4*4)], axis=0)#np.stack([astronaut() for i in range(4*4)], axis=0)\n",
    "\n",
    "data_gen = ImageDataGenerator(\n",
    "    width_shift_range = 0.1,height_shift_range = 0.1,#rescale=1./255.,\n",
    "    fill_mode='nearest',validation_split=0.25,horizontal_flip = True,rotation_range = 90,\n",
    "    preprocessing_function=lambda x: x[..., np.random.permutation([0, 1, 2])]\n",
    ")\n",
    "fig = plt.figure()\n",
    "plt.subplots_adjust(wspace=.2, hspace=.2)\n",
    "for index, image in enumerate(next(data_gen.flow(imgs)).astype(int)):\n",
    "    ax = plt.subplot(4, 4, index + 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./DenseNet_cifar10.h5', save_weights_only=False,save_best_only=True, \\\n",
    "                                       mode='max', monitor='val_accuracy',verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=2,mode='max',verbose=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y5rMTCIyGB24",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()#SGD(learning_rate=0.1,momentum=0.9,)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "781/781 [==============================] - 263s 306ms/step - loss: 1.8543 - accuracy: 0.3461 - val_loss: 1.8558 - val_accuracy: 0.4345\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.43450, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 1.4837 - accuracy: 0.4686 - val_loss: 1.4215 - val_accuracy: 0.4928\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.43450 to 0.49279, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 1.2536 - accuracy: 0.5520 - val_loss: 1.3083 - val_accuracy: 0.5240\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.49279 to 0.52404, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 1.0839 - accuracy: 0.6172 - val_loss: 1.1843 - val_accuracy: 0.5964\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.52404 to 0.59635, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.9422 - accuracy: 0.6718 - val_loss: 1.0142 - val_accuracy: 0.6478\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.59635 to 0.64784, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.8570 - accuracy: 0.7039 - val_loss: 0.9050 - val_accuracy: 0.6841\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.64784 to 0.68409, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.7892 - accuracy: 0.7305 - val_loss: 0.8315 - val_accuracy: 0.7222\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.68409 to 0.72216, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.7338 - accuracy: 0.7484 - val_loss: 0.7876 - val_accuracy: 0.7336\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.72216 to 0.73357, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.6884 - accuracy: 0.7657 - val_loss: 0.8014 - val_accuracy: 0.7296\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.73357\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.6426 - accuracy: 0.7846 - val_loss: 0.7702 - val_accuracy: 0.7476\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.73357 to 0.74760, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.5976 - accuracy: 0.7974 - val_loss: 0.6578 - val_accuracy: 0.7788\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.74760 to 0.77885, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.5759 - accuracy: 0.8046 - val_loss: 0.6731 - val_accuracy: 0.7722\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.77885\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.5455 - accuracy: 0.8177 - val_loss: 0.6098 - val_accuracy: 0.7895\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.77885 to 0.78946, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.5335 - accuracy: 0.8209 - val_loss: 0.5867 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.78946 to 0.79708, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.5065 - accuracy: 0.8312 - val_loss: 0.5682 - val_accuracy: 0.8073\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.79708 to 0.80729, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.4873 - accuracy: 0.8346 - val_loss: 0.5602 - val_accuracy: 0.8079\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.80729 to 0.80789, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.4601 - accuracy: 0.8439 - val_loss: 0.5451 - val_accuracy: 0.8199\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.80789 to 0.81991, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.4430 - accuracy: 0.8517 - val_loss: 0.5576 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.81991\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.4369 - accuracy: 0.8540 - val_loss: 0.5101 - val_accuracy: 0.8317\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.81991 to 0.83173, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.4170 - accuracy: 0.8600 - val_loss: 0.4856 - val_accuracy: 0.8379\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.83173 to 0.83794, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.4054 - accuracy: 0.8638 - val_loss: 0.5182 - val_accuracy: 0.8305\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.83794\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.3970 - accuracy: 0.8656 - val_loss: 0.4723 - val_accuracy: 0.8377\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.83794\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.3190 - accuracy: 0.8948 - val_loss: 0.3742 - val_accuracy: 0.8766\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.83794 to 0.87660, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2849 - accuracy: 0.9060 - val_loss: 0.3608 - val_accuracy: 0.8836\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.87660 to 0.88361, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2755 - accuracy: 0.9090 - val_loss: 0.3631 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.88361\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2723 - accuracy: 0.9090 - val_loss: 0.3849 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.88361\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2683 - accuracy: 0.9098 - val_loss: 0.3379 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.88361 to 0.88662, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2633 - accuracy: 0.9117 - val_loss: 0.3514 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.88662 to 0.88782, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2597 - accuracy: 0.9126 - val_loss: 0.3581 - val_accuracy: 0.8840\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.88782\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2585 - accuracy: 0.9120 - val_loss: 0.3352 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.88782 to 0.89463, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2603 - accuracy: 0.9137 - val_loss: 0.3497 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.89463\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2594 - accuracy: 0.9123 - val_loss: 0.3626 - val_accuracy: 0.8808\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2554 - accuracy: 0.9136 - val_loss: 0.3709 - val_accuracy: 0.8810\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.89463\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2580 - accuracy: 0.9128 - val_loss: 0.3469 - val_accuracy: 0.8864\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2499 - accuracy: 0.9159 - val_loss: 0.3448 - val_accuracy: 0.8926\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.89463\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2543 - accuracy: 0.9164 - val_loss: 0.3321 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2468 - accuracy: 0.9178 - val_loss: 0.3656 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.89463\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2513 - accuracy: 0.9162 - val_loss: 0.3359 - val_accuracy: 0.8924\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2565 - accuracy: 0.9149 - val_loss: 0.3485 - val_accuracy: 0.8836\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.89463\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 229s 294ms/step - loss: 0.2572 - accuracy: 0.9140 - val_loss: 0.3481 - val_accuracy: 0.8836\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2513 - accuracy: 0.9141 - val_loss: 0.3471 - val_accuracy: 0.8824\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.89463\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2507 - accuracy: 0.9178 - val_loss: 0.3601 - val_accuracy: 0.8784\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2551 - accuracy: 0.9166 - val_loss: 0.3472 - val_accuracy: 0.8864\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.89463\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2537 - accuracy: 0.9134 - val_loss: 0.3515 - val_accuracy: 0.8852\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2550 - accuracy: 0.9153 - val_loss: 0.3658 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.89463\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2548 - accuracy: 0.9159 - val_loss: 0.3830 - val_accuracy: 0.8738\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2456 - accuracy: 0.9182 - val_loss: 0.3490 - val_accuracy: 0.8808\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.89463\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2569 - accuracy: 0.9146 - val_loss: 0.3475 - val_accuracy: 0.8870\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2479 - accuracy: 0.9156 - val_loss: 0.3721 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.89463\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2514 - accuracy: 0.9158 - val_loss: 0.3501 - val_accuracy: 0.8824\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2532 - accuracy: 0.9140 - val_loss: 0.3545 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.89463\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 230s 294ms/step - loss: 0.2554 - accuracy: 0.9142 - val_loss: 0.3532 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 228s 292ms/step - loss: 0.2545 - accuracy: 0.9149 - val_loss: 0.3643 - val_accuracy: 0.8808\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.89463\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 228s 292ms/step - loss: 0.2527 - accuracy: 0.9164 - val_loss: 0.3526 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2554 - accuracy: 0.9152 - val_loss: 0.3405 - val_accuracy: 0.8868\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.89463\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2538 - accuracy: 0.9145 - val_loss: 0.3368 - val_accuracy: 0.8896\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2548 - accuracy: 0.9144 - val_loss: 0.3500 - val_accuracy: 0.8890\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.89463\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2565 - accuracy: 0.9165 - val_loss: 0.3551 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2509 - accuracy: 0.9166 - val_loss: 0.3462 - val_accuracy: 0.8888\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.89463\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2581 - accuracy: 0.9150 - val_loss: 0.3616 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2522 - accuracy: 0.9163 - val_loss: 0.3485 - val_accuracy: 0.8850\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.89463\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2505 - accuracy: 0.9164 - val_loss: 0.3417 - val_accuracy: 0.8842\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2564 - accuracy: 0.9156 - val_loss: 0.3543 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.89463\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2540 - accuracy: 0.9141 - val_loss: 0.3333 - val_accuracy: 0.8914\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2566 - accuracy: 0.9152 - val_loss: 0.3578 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.89463\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2561 - accuracy: 0.9138 - val_loss: 0.3514 - val_accuracy: 0.8828\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2537 - accuracy: 0.9163 - val_loss: 0.3550 - val_accuracy: 0.8820\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.89463\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2568 - accuracy: 0.9136 - val_loss: 0.3537 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2521 - accuracy: 0.9155 - val_loss: 0.3536 - val_accuracy: 0.8818\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.89463\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2566 - accuracy: 0.9161 - val_loss: 0.3380 - val_accuracy: 0.8860\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.0000001181490946e-25.\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2551 - accuracy: 0.9138 - val_loss: 0.3376 - val_accuracy: 0.8856\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.89463\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2572 - accuracy: 0.9159 - val_loss: 0.3566 - val_accuracy: 0.8836\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.0000001428009978e-26.\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2558 - accuracy: 0.9141 - val_loss: 0.3560 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.89463\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2584 - accuracy: 0.9145 - val_loss: 0.3551 - val_accuracy: 0.8840\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.000000142800998e-27.\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2479 - accuracy: 0.9166 - val_loss: 0.3599 - val_accuracy: 0.8824\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.89463\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2565 - accuracy: 0.9150 - val_loss: 0.3590 - val_accuracy: 0.8826\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.0000001235416984e-28.\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2545 - accuracy: 0.9177 - val_loss: 0.3699 - val_accuracy: 0.8836\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.89463\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2564 - accuracy: 0.9144 - val_loss: 0.3579 - val_accuracy: 0.8828\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000001235416985e-29.\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2569 - accuracy: 0.9161 - val_loss: 0.3277 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.89463\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2570 - accuracy: 0.9148 - val_loss: 0.3532 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.0000001536343539e-30.\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2481 - accuracy: 0.9160 - val_loss: 0.3728 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.89463\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2521 - accuracy: 0.9166 - val_loss: 0.3572 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.000000191250173e-31.\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2602 - accuracy: 0.9114 - val_loss: 0.3463 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.89463\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2546 - accuracy: 0.9147 - val_loss: 0.3639 - val_accuracy: 0.8782\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.0000002147600601e-32.\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2542 - accuracy: 0.9144 - val_loss: 0.3540 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.89463\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2549 - accuracy: 0.9153 - val_loss: 0.3385 - val_accuracy: 0.8882\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.0000002441474188e-33.\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2507 - accuracy: 0.9155 - val_loss: 0.3434 - val_accuracy: 0.8876\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.89463\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2497 - accuracy: 0.9165 - val_loss: 0.3622 - val_accuracy: 0.8834\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.0000002074132203e-34.\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2569 - accuracy: 0.9134 - val_loss: 0.3544 - val_accuracy: 0.8884\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.89463\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2535 - accuracy: 0.9152 - val_loss: 0.3569 - val_accuracy: 0.8808\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.0000001614954722e-35.\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2600 - accuracy: 0.9143 - val_loss: 0.3606 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.89463\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2486 - accuracy: 0.9169 - val_loss: 0.3525 - val_accuracy: 0.8834\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1.0000001614954723e-36.\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2505 - accuracy: 0.9174 - val_loss: 0.3741 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.89463\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2536 - accuracy: 0.9151 - val_loss: 0.3524 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.0000001256222317e-37.\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2524 - accuracy: 0.9163 - val_loss: 0.3461 - val_accuracy: 0.8908\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.89463\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2597 - accuracy: 0.9137 - val_loss: 0.3563 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.0000001032014561e-38.\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2559 - accuracy: 0.9143 - val_loss: 0.3479 - val_accuracy: 0.8868\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.89463\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2534 - accuracy: 0.9148 - val_loss: 0.3655 - val_accuracy: 0.8816\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 1.0000000751754869e-39.\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2533 - accuracy: 0.9156 - val_loss: 0.3482 - val_accuracy: 0.8820\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.89463\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 229s 293ms/step - loss: 0.2558 - accuracy: 0.9151 - val_loss: 0.3455 - val_accuracy: 0.8818\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.0000002153053334e-40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f5ecbf100>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=tf.keras.optimizers.SGD(learning_rate = 0.1,momentum = 0.9,nesterov = True), #Adam(),\n",
    "#              metrics=['accuracy'])\n",
    "model.fit(train_datagen.flow(X_train, Y_train,),steps_per_epoch=steps,\n",
    "          validation_data=test_datagen.flow(X_test, Y_test),validation_steps=val_steps,\n",
    "          \n",
    "          epochs=100,\n",
    "          callbacks=callbacks,\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MODEL\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m=load_model('./MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/390 [==============================] - 138s 297ms/step - loss: 0.2585 - accuracy: 0.9179 - val_loss: 0.4030 - val_accuracy: 0.8706\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.87059, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 118s 304ms/step - loss: 0.2503 - accuracy: 0.9178 - val_loss: 0.3686 - val_accuracy: 0.8826\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.87059 to 0.88261, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 119s 305ms/step - loss: 0.2626 - accuracy: 0.9120 - val_loss: 0.3751 - val_accuracy: 0.8814\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.88261\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 115s 295ms/step - loss: 0.2528 - accuracy: 0.9135 - val_loss: 0.3635 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.88261 to 0.88982, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 120s 307ms/step - loss: 0.2461 - accuracy: 0.9170 - val_loss: 0.3431 - val_accuracy: 0.8882\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.88982\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 107s 275ms/step - loss: 0.2513 - accuracy: 0.9160 - val_loss: 0.3615 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.88982\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 120s 308ms/step - loss: 0.2443 - accuracy: 0.9167 - val_loss: 0.3361 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.88982 to 0.89183, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 118s 301ms/step - loss: 0.2368 - accuracy: 0.9185 - val_loss: 0.3465 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.89183\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2466 - accuracy: 0.9196 - val_loss: 0.3577 - val_accuracy: 0.8822\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.89183\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2468 - accuracy: 0.9182 - val_loss: 0.3547 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.89183\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2450 - accuracy: 0.9196 - val_loss: 0.3443 - val_accuracy: 0.8906\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.89183\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2402 - accuracy: 0.9196 - val_loss: 0.3464 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.89183\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2368 - accuracy: 0.9203 - val_loss: 0.3509 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.89183\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2429 - accuracy: 0.9186 - val_loss: 0.3531 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.89183\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2458 - accuracy: 0.9192 - val_loss: 0.3638 - val_accuracy: 0.8826\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.89183\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2320 - accuracy: 0.9247 - val_loss: 0.3391 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.89183 to 0.89784, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2443 - accuracy: 0.9175 - val_loss: 0.3751 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.89784\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2455 - accuracy: 0.9203 - val_loss: 0.3351 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2333 - accuracy: 0.9205 - val_loss: 0.3674 - val_accuracy: 0.8794\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.89784\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2458 - accuracy: 0.9179 - val_loss: 0.3572 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2432 - accuracy: 0.9182 - val_loss: 0.3224 - val_accuracy: 0.8974\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.89784\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2461 - accuracy: 0.9174 - val_loss: 0.3715 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2432 - accuracy: 0.9194 - val_loss: 0.3583 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.89784\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2421 - accuracy: 0.9186 - val_loss: 0.3578 - val_accuracy: 0.8814\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2389 - accuracy: 0.9213 - val_loss: 0.3135 - val_accuracy: 0.8942\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.89784\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2322 - accuracy: 0.9243 - val_loss: 0.3347 - val_accuracy: 0.8854\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2385 - accuracy: 0.9200 - val_loss: 0.3569 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.89784\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2447 - accuracy: 0.9206 - val_loss: 0.3292 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2464 - accuracy: 0.9174 - val_loss: 0.3319 - val_accuracy: 0.8874\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.89784\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2456 - accuracy: 0.9183 - val_loss: 0.3302 - val_accuracy: 0.8926\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2320 - accuracy: 0.9230 - val_loss: 0.3608 - val_accuracy: 0.8842\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.89784\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2448 - accuracy: 0.9188 - val_loss: 0.3504 - val_accuracy: 0.8854\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2441 - accuracy: 0.9188 - val_loss: 0.3301 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.89784\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2439 - accuracy: 0.9171 - val_loss: 0.3643 - val_accuracy: 0.8874\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2379 - accuracy: 0.9225 - val_loss: 0.3382 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.89784\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2442 - accuracy: 0.9173 - val_loss: 0.3850 - val_accuracy: 0.8794\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2344 - accuracy: 0.9202 - val_loss: 0.3574 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.89784\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2395 - accuracy: 0.9180 - val_loss: 0.3658 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2406 - accuracy: 0.9192 - val_loss: 0.3516 - val_accuracy: 0.8794\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.89784\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2468 - accuracy: 0.9186 - val_loss: 0.3633 - val_accuracy: 0.8794\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2399 - accuracy: 0.9225 - val_loss: 0.3398 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.89784\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2407 - accuracy: 0.9170 - val_loss: 0.3707 - val_accuracy: 0.8806\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2406 - accuracy: 0.9185 - val_loss: 0.3504 - val_accuracy: 0.8890\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.89784\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2412 - accuracy: 0.9212 - val_loss: 0.3354 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.999999682655227e-24.\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2403 - accuracy: 0.9196 - val_loss: 0.3697 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.89784\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2369 - accuracy: 0.9252 - val_loss: 0.3503 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.999999998199588e-25.\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2345 - accuracy: 0.9231 - val_loss: 0.3547 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.89784\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2473 - accuracy: 0.9199 - val_loss: 0.3207 - val_accuracy: 0.8942\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-25.\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2426 - accuracy: 0.9208 - val_loss: 0.3477 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.89784\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2346 - accuracy: 0.9215 - val_loss: 0.3398 - val_accuracy: 0.8954\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-26.\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2403 - accuracy: 0.9177 - val_loss: 0.3515 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.89784\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2434 - accuracy: 0.9167 - val_loss: 0.3488 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 9.999999887266024e-28.\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2349 - accuracy: 0.9245 - val_loss: 0.3532 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.89784\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2459 - accuracy: 0.9193 - val_loss: 0.3228 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.0000000272452012e-28.\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2403 - accuracy: 0.9196 - val_loss: 0.3262 - val_accuracy: 0.8882\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.89784\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2430 - accuracy: 0.9194 - val_loss: 0.3813 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-29.\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2445 - accuracy: 0.9196 - val_loss: 0.3430 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.89784\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2470 - accuracy: 0.9170 - val_loss: 0.3739 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-30.\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2412 - accuracy: 0.9187 - val_loss: 0.3648 - val_accuracy: 0.8802\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.89784\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2394 - accuracy: 0.9200 - val_loss: 0.3654 - val_accuracy: 0.8882\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.000000003171077e-31.\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2374 - accuracy: 0.9189 - val_loss: 0.3570 - val_accuracy: 0.8886\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.89784\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2451 - accuracy: 0.9176 - val_loss: 0.3542 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 9.999999796611899e-33.\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2448 - accuracy: 0.9193 - val_loss: 0.3498 - val_accuracy: 0.8890\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.89784\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2536 - accuracy: 0.9158 - val_loss: 0.3491 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 9.999999502738312e-34.\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2409 - accuracy: 0.9184 - val_loss: 0.3493 - val_accuracy: 0.8826\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.89784\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2490 - accuracy: 0.9155 - val_loss: 0.3100 - val_accuracy: 0.8934\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 9.999999319067318e-35.\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2353 - accuracy: 0.9206 - val_loss: 0.3704 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.89784\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2340 - accuracy: 0.9229 - val_loss: 0.3117 - val_accuracy: 0.8974\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.89784\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 9.999999319067319e-36.\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2399 - accuracy: 0.9203 - val_loss: 0.3424 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.89784\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2424 - accuracy: 0.9186 - val_loss: 0.3215 - val_accuracy: 0.8982\n",
      "\n",
      "Epoch 00070: val_accuracy improved from 0.89784 to 0.89824, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2382 - accuracy: 0.9208 - val_loss: 0.3300 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.89824\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2443 - accuracy: 0.9204 - val_loss: 0.3450 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.89824\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 9.999999462560281e-37.\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2491 - accuracy: 0.9167 - val_loss: 0.3368 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.89824\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2455 - accuracy: 0.9205 - val_loss: 0.3074 - val_accuracy: 0.8966\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.89824\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 9.99999946256028e-38.\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2435 - accuracy: 0.9177 - val_loss: 0.3452 - val_accuracy: 0.8914\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.89824\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2446 - accuracy: 0.9198 - val_loss: 0.3196 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.89824\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 9.99999991097579e-39.\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2392 - accuracy: 0.9216 - val_loss: 0.3527 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.89824\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2316 - accuracy: 0.9228 - val_loss: 0.3641 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.89824\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 9.999999350456405e-40.\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2419 - accuracy: 0.9187 - val_loss: 0.3261 - val_accuracy: 0.8938\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.89824\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2497 - accuracy: 0.9189 - val_loss: 0.3478 - val_accuracy: 0.8826\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.89824\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.0000002153053334e-40.\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 0.2423 - accuracy: 0.9207 - val_loss: 0.3497 - val_accuracy: 0.8818\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.89824\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2448 - accuracy: 0.9177 - val_loss: 0.3220 - val_accuracy: 0.9018\n",
      "\n",
      "Epoch 00082: val_accuracy improved from 0.89824 to 0.90184, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2387 - accuracy: 0.9207 - val_loss: 0.3183 - val_accuracy: 0.8906\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.90184\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2379 - accuracy: 0.9209 - val_loss: 0.3507 - val_accuracy: 0.8826\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.90184\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 9.99994610111476e-42.\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 115s 294ms/step - loss: 0.2261 - accuracy: 0.9240 - val_loss: 0.3484 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.90184\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 117s 301ms/step - loss: 0.2461 - accuracy: 0.9166 - val_loss: 0.3558 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.90184\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 9.999665841421895e-43.\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 117s 301ms/step - loss: 0.2472 - accuracy: 0.9162 - val_loss: 0.3413 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.90184\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 115s 295ms/step - loss: 0.2342 - accuracy: 0.9216 - val_loss: 0.3588 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.90184\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.0005271035279195e-43.\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 115s 295ms/step - loss: 0.2365 - accuracy: 0.9211 - val_loss: 0.3379 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.90184\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 117s 300ms/step - loss: 0.2434 - accuracy: 0.9176 - val_loss: 0.3364 - val_accuracy: 0.8914\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.90184\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 9.949219096706202e-45.\n",
      "Epoch 91/100\n",
      " 30/390 [=>............................] - ETA: 1:45 - loss: 0.2158 - accuracy: 0.9208"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m opt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\u001b[38;5;66;03m#SGD(learning_rate=0.1,momentum=0.9,)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m m\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mopt,\n\u001b[0;32m     10\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m          \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m         \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\engine\\training.py:1189\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1187\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1191\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\callbacks.py:435\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 435\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\callbacks.py:315\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    312\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    313\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    318\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\callbacks.py:353\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    352\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 353\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    356\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\callbacks.py:1028\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1028\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\callbacks.py:1100\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1096\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\utils\\tf_utils.py:516\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\keras\\utils\\tf_utils.py:512\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    511\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 512\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1094\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf_GPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1060\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./DenseNet_cifar10.h5', save_weights_only=False,save_best_only=True, \\\n",
    "                                       mode='max', monitor='val_accuracy',verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=2,mode='max',verbose=1),\n",
    "]\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)#SGD(learning_rate=0.1,momentum=0.9,)\n",
    "m.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "m.fit(train_datagen.flow(X_train, Y_train,),steps_per_epoch=steps,\n",
    "          validation_data=test_datagen.flow(X_test, Y_test),validation_steps=val_steps,\n",
    "          \n",
    "          epochs=100,\n",
    "          callbacks=callbacks,\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MODEL2\\assets\n"
     ]
    }
   ],
   "source": [
    "m.save('./MODEL2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m1=load_model('./MODEL2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/390 [==============================] - 119s 294ms/step - loss: 0.2439 - accuracy: 0.9220 - val_loss: 0.3418 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.88662, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2327 - accuracy: 0.9207 - val_loss: 0.3189 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.88662 to 0.89223, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2271 - accuracy: 0.9251 - val_loss: 0.3593 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89223\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2281 - accuracy: 0.9231 - val_loss: 0.3550 - val_accuracy: 0.8854\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89223\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 122s 312ms/step - loss: 0.2367 - accuracy: 0.9204 - val_loss: 0.3594 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89223\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2450 - accuracy: 0.9192 - val_loss: 0.3813 - val_accuracy: 0.8794\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.89223\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2455 - accuracy: 0.9205 - val_loss: 0.3206 - val_accuracy: 0.8926\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.89223 to 0.89263, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2396 - accuracy: 0.9206 - val_loss: 0.3399 - val_accuracy: 0.8874\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.89263\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2468 - accuracy: 0.9163 - val_loss: 0.3493 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.89263\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 0.2448 - accuracy: 0.9163 - val_loss: 0.3401 - val_accuracy: 0.8810\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.89263\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 131s 335ms/step - loss: 0.2432 - accuracy: 0.9211 - val_loss: 0.3562 - val_accuracy: 0.8850\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.89263\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 139s 356ms/step - loss: 0.2408 - accuracy: 0.9193 - val_loss: 0.3598 - val_accuracy: 0.8786\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.89263\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 140s 359ms/step - loss: 0.2416 - accuracy: 0.9232 - val_loss: 0.3312 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.89263 to 0.89463, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 139s 356ms/step - loss: 0.2376 - accuracy: 0.9225 - val_loss: 0.3526 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.89463\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 139s 358ms/step - loss: 0.2328 - accuracy: 0.9200 - val_loss: 0.3695 - val_accuracy: 0.8822\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 139s 356ms/step - loss: 0.2423 - accuracy: 0.9218 - val_loss: 0.3409 - val_accuracy: 0.8870\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.89463\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 129s 331ms/step - loss: 0.2353 - accuracy: 0.9216 - val_loss: 0.3393 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 114s 293ms/step - loss: 0.2314 - accuracy: 0.9231 - val_loss: 0.3431 - val_accuracy: 0.8906\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.89463\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.2412 - accuracy: 0.9168 - val_loss: 0.3663 - val_accuracy: 0.8814\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2455 - accuracy: 0.9213 - val_loss: 0.3516 - val_accuracy: 0.8762\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.89463\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2370 - accuracy: 0.9198 - val_loss: 0.3563 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.89463\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2351 - accuracy: 0.9215 - val_loss: 0.3396 - val_accuracy: 0.8942\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.89463\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2300 - accuracy: 0.9237 - val_loss: 0.3246 - val_accuracy: 0.8950\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.89463 to 0.89503, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2389 - accuracy: 0.9202 - val_loss: 0.3470 - val_accuracy: 0.8854\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.89503\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2335 - accuracy: 0.9216 - val_loss: 0.3149 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.89503 to 0.89583, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2405 - accuracy: 0.9185 - val_loss: 0.3545 - val_accuracy: 0.8870\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.89583\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2414 - accuracy: 0.9204 - val_loss: 0.3177 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.89583\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2291 - accuracy: 0.9236 - val_loss: 0.3385 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.89583\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2307 - accuracy: 0.9229 - val_loss: 0.3457 - val_accuracy: 0.8938\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.89583\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2361 - accuracy: 0.9250 - val_loss: 0.3135 - val_accuracy: 0.8974\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.89583 to 0.89744, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2590 - accuracy: 0.9143 - val_loss: 0.3423 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.89744\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2483 - accuracy: 0.9201 - val_loss: 0.3110 - val_accuracy: 0.8934\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2383 - accuracy: 0.9192 - val_loss: 0.3355 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.89744\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2373 - accuracy: 0.9212 - val_loss: 0.3495 - val_accuracy: 0.8894\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2433 - accuracy: 0.9193 - val_loss: 0.3250 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.89744\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2408 - accuracy: 0.9187 - val_loss: 0.3423 - val_accuracy: 0.8950\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2412 - accuracy: 0.9202 - val_loss: 0.3599 - val_accuracy: 0.8766\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.89744\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2482 - accuracy: 0.9196 - val_loss: 0.3704 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2415 - accuracy: 0.9183 - val_loss: 0.3618 - val_accuracy: 0.8818\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.89744\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2438 - accuracy: 0.9187 - val_loss: 0.3381 - val_accuracy: 0.8934\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2320 - accuracy: 0.9232 - val_loss: 0.3426 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.89744\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2452 - accuracy: 0.9168 - val_loss: 0.3194 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2431 - accuracy: 0.9186 - val_loss: 0.3615 - val_accuracy: 0.8810\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.89744\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2498 - accuracy: 0.9139 - val_loss: 0.3475 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.999999682655227e-24.\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 82s 211ms/step - loss: 0.2338 - accuracy: 0.9218 - val_loss: 0.3284 - val_accuracy: 0.8914\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.89744\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2390 - accuracy: 0.9198 - val_loss: 0.3461 - val_accuracy: 0.8850\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.999999998199588e-25.\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2517 - accuracy: 0.9158 - val_loss: 0.3464 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.89744\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2459 - accuracy: 0.9175 - val_loss: 0.3433 - val_accuracy: 0.8886\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-25.\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2427 - accuracy: 0.9203 - val_loss: 0.3609 - val_accuracy: 0.8834\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.89744\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2412 - accuracy: 0.9203 - val_loss: 0.3678 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.89744\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-26.\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2479 - accuracy: 0.9189 - val_loss: 0.3719 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.89744\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2300 - accuracy: 0.9232 - val_loss: 0.3176 - val_accuracy: 0.8986\n",
      "\n",
      "Epoch 00052: val_accuracy improved from 0.89744 to 0.89864, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2333 - accuracy: 0.9238 - val_loss: 0.3634 - val_accuracy: 0.8798\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.89864\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2394 - accuracy: 0.9179 - val_loss: 0.2890 - val_accuracy: 0.9087\n",
      "\n",
      "Epoch 00054: val_accuracy improved from 0.89864 to 0.90865, saving model to .\\DenseNet_cifar10.h5\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2367 - accuracy: 0.9227 - val_loss: 0.3337 - val_accuracy: 0.8938\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.90865\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2397 - accuracy: 0.9200 - val_loss: 0.3611 - val_accuracy: 0.8842\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999999887266024e-28.\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2378 - accuracy: 0.9209 - val_loss: 0.3732 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.90865\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2258 - accuracy: 0.9233 - val_loss: 0.3313 - val_accuracy: 0.8938\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000272452012e-28.\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2442 - accuracy: 0.9193 - val_loss: 0.3444 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.90865\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2518 - accuracy: 0.9162 - val_loss: 0.3626 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-29.\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2394 - accuracy: 0.9181 - val_loss: 0.2981 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.90865\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2402 - accuracy: 0.9214 - val_loss: 0.3361 - val_accuracy: 0.8950\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-30.\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2393 - accuracy: 0.9235 - val_loss: 0.3067 - val_accuracy: 0.8982\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.90865\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2356 - accuracy: 0.9200 - val_loss: 0.3328 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.000000003171077e-31.\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2407 - accuracy: 0.9187 - val_loss: 0.3645 - val_accuracy: 0.8870\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.90865\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2487 - accuracy: 0.9183 - val_loss: 0.3375 - val_accuracy: 0.8818\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 9.999999796611899e-33.\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2405 - accuracy: 0.9204 - val_loss: 0.3454 - val_accuracy: 0.8870\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.90865\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2394 - accuracy: 0.9220 - val_loss: 0.3221 - val_accuracy: 0.9018\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 9.999999502738312e-34.\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2363 - accuracy: 0.9224 - val_loss: 0.3656 - val_accuracy: 0.8814\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.90865\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2501 - accuracy: 0.9173 - val_loss: 0.3303 - val_accuracy: 0.8950\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 9.999999319067318e-35.\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2432 - accuracy: 0.9185 - val_loss: 0.3347 - val_accuracy: 0.8886\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.90865\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2448 - accuracy: 0.9179 - val_loss: 0.3451 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 9.999999319067319e-36.\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2436 - accuracy: 0.9178 - val_loss: 0.3530 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.90865\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2300 - accuracy: 0.9232 - val_loss: 0.3625 - val_accuracy: 0.8814\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 9.999999462560281e-37.\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2401 - accuracy: 0.9204 - val_loss: 0.3444 - val_accuracy: 0.8870\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.90865\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2349 - accuracy: 0.9198 - val_loss: 0.3433 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 9.99999946256028e-38.\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2476 - accuracy: 0.9182 - val_loss: 0.3599 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.90865\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2437 - accuracy: 0.9169 - val_loss: 0.3297 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 9.99999991097579e-39.\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2364 - accuracy: 0.9208 - val_loss: 0.3430 - val_accuracy: 0.8870\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.90865\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2375 - accuracy: 0.9217 - val_loss: 0.3365 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 9.999999350456405e-40.\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2390 - accuracy: 0.9190 - val_loss: 0.3353 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.90865\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2380 - accuracy: 0.9218 - val_loss: 0.3318 - val_accuracy: 0.8894\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.0000002153053334e-40.\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2538 - accuracy: 0.9140 - val_loss: 0.3476 - val_accuracy: 0.8814\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.90865\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2365 - accuracy: 0.9227 - val_loss: 0.3237 - val_accuracy: 0.8934\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 9.99994610111476e-42.\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2441 - accuracy: 0.9179 - val_loss: 0.2940 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.90865\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2414 - accuracy: 0.9192 - val_loss: 0.3461 - val_accuracy: 0.8842\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 9.999665841421895e-43.\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2357 - accuracy: 0.9238 - val_loss: 0.3718 - val_accuracy: 0.8754\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.90865\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2377 - accuracy: 0.9223 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.0005271035279195e-43.\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2296 - accuracy: 0.9245 - val_loss: 0.3196 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.90865\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2493 - accuracy: 0.9173 - val_loss: 0.3201 - val_accuracy: 0.8906\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 9.949219096706202e-45.\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2417 - accuracy: 0.9186 - val_loss: 0.3437 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.90865\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2464 - accuracy: 0.9176 - val_loss: 0.3590 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 9.80908925027372e-46.\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2423 - accuracy: 0.9195 - val_loss: 0.3484 - val_accuracy: 0.8874\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.90865\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2302 - accuracy: 0.9227 - val_loss: 0.3073 - val_accuracy: 0.9006\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.90865\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.4012984643248171e-46.\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2333 - accuracy: 0.9227 - val_loss: 0.3456 - val_accuracy: 0.8898\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.90865\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2415 - accuracy: 0.9192 - val_loss: 0.3248 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.90865\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2423 - accuracy: 0.9185 - val_loss: 0.3296 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.90865\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2443 - accuracy: 0.9181 - val_loss: 0.2993 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.90865\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 0.2419 - accuracy: 0.9208 - val_loss: 0.3662 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.90865\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 0.2377 - accuracy: 0.9220 - val_loss: 0.3673 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.90865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aefe27e040>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./DenseNet_cifar10.h5', save_weights_only=False,save_best_only=True, \\\n",
    "                                       mode='max', monitor='val_accuracy',verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=2,mode='max',verbose=1),\n",
    "]\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.000001)#SGD(learning_rate=0.1,momentum=0.9,)\n",
    "m1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "m1.fit(train_datagen.flow(X_train, Y_train,),steps_per_epoch=steps,\n",
    "          validation_data=test_datagen.flow(X_test, Y_test),validation_steps=val_steps,\n",
    "          epochs=100,\n",
    "          callbacks=callbacks,\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-aJp6LhXjLR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DenseNet - cifar10.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
