{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K77aHmovDF-5"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0Hv18ozDMW-"
   },
   "source": [
    "A. Compute performance metrics for the given data 5_a.csv\n",
    "    \n",
    "    Note 1. In this data you can see number of positive points >> number of negatives points\n",
    "    Note 2. Use pandas or numpy to read the data from <b>5_a.csv\n",
    "    Note 3. You need to derive the class labels from given score\n",
    "   $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "  \n",
    "a. Compute Confusion Matrix \n",
    "\n",
    "b. Compute F1 Score \n",
    "\n",
    "c. Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then\n",
    "   use numpy.trapz(tpr_array, fpr_array) \n",
    "\n",
    "d. Compute Accuracy Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhprcJ0-DW6i"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "NlYkAEVpDOsu",
    "outputId": "6b1b3b7b-beb6-46d8-da3c-5da11060e971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DpEo5-U0EWSs"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(df):\n",
    "  tp,tn,fp,fn = [],[],[],[]\n",
    "  for indx,row in (df.iterrows()):\n",
    "    if row['y_score']==1 and row['y']==1:\n",
    "      tp.append(df.iloc[indx])\n",
    "    elif row['y_score']==0 and row['y']==0:\n",
    "      tn.append(df.iloc[indx])\n",
    "    elif row['y_score']==1 and row['y']==0:\n",
    "      fp.append(df.iloc[indx])\n",
    "    else: #row['y_score']==0 and row['y']==1:\n",
    "      fn.append(df.iloc[indx])\n",
    "  #print(len(tn),len(fn),len(fp),len(tp))\n",
    "  #conf_matrix = [len(tn),len(fn),len(fp),len(tp)]\n",
    "  conf_matrix = np.reshape([len(tn),len(fn),len(fp),len(tp)],(2,2))\n",
    "  acc_score = (len(tn)+len(tp))/(len(tn)+len(fn)+len(fp)+len(tp))\n",
    "  return conf_matrix,acc_score\n",
    "\n",
    "def f1_score(df):\n",
    "  precision = df[df['y_score']==1]\n",
    "  pr = precision[precision['y']==1].shape[0]/precision.shape[0]\n",
    "  print('precision :',pr,'\\n')\n",
    "\n",
    "  recall = df[df['y']==1]\n",
    "  rc = recall[recall['y_score']==1].shape[0]/recall.shape[0]\n",
    "  print('recall :',rc,'\\n')\n",
    "\n",
    "  f1_sc = 2*((pr*rc)/(pr+rc))\n",
    "  #print('f1_score :',f1_sc,'\\n')\n",
    "  return f1_sc\n",
    "\n",
    "def AUC_score(df):\n",
    "  x = df.sort_values(by=['proba'],ascending=False,ignore_index=True)\n",
    "  x['y_score'] = 0\n",
    "  D_tpr = dict()\n",
    "  D_fpr = dict()\n",
    "  for i in range(len(x)):\n",
    "    x['y_score'].iloc[i] = 1\n",
    "    d  = dict(x['y_score'].value_counts())\n",
    "    \n",
    "    d1 = dict(x['y'].iloc[:i].value_counts())\n",
    "    #print('fp',d1.get(0,0))\n",
    "    #print('tp',d1.get(1,0))\n",
    "    \n",
    "    d2 = dict(x['y'].iloc[i:].value_counts())\n",
    "    #print('tn',d2.get(0,0))\n",
    "    #print('fn',d2.get(1,0))\n",
    "\n",
    "    P = d1.get(1,0) + d2.get(1,0)\n",
    "    N = d2.get(0,0) + d1.get(0,0)\n",
    "\n",
    "    tpr = (d1.get(1,0))/P\n",
    "    fpr = (d1.get(0,0))/N\n",
    "    D_tpr[x['proba'].iloc[i]]=tpr\n",
    "    D_fpr[x['proba'].iloc[i]]=fpr\n",
    "  #print(D_tpr,'\\n',len(D_tpr),'\\n',df.shape[0],'\\n',D_fpr,'\\n',len(D_fpr))\n",
    "  fpr_array = np.array(list(D_fpr.values()))\n",
    "  tpr_array = np.array(list(D_tpr.values()))\n",
    "  auc_score = np.trapz(tpr_array, fpr_array)\n",
    "  print(len(D_tpr),len(D_fpr))\n",
    "  return auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "kvbgc9Z1VXz_",
    "outputId": "6b21d71e-b09a-45c8-e9e9-3a2688d4504e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion-Matrix :\n",
      " [[    0     0]\n",
      " [  100 10000]] \n",
      "\n",
      "precision : 0.9900990099009901 \n",
      "\n",
      "recall : 1.0 \n",
      "\n",
      "f1_score :  0.9950248756218906 \n",
      "\n",
      "10100 10100\n",
      "AUC_Score : 0.48829900000000004 \n",
      "\n",
      "Accuraccy_Score : 0.9900990099009901 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('/content/gdrive/My Drive/AAIC/Assignments/7.Compute Performance metrics without Sklearn/Copy of 5_a.csv')\n",
    "df1['y_score']=0\n",
    "for indx,row in df1.iterrows():\n",
    "  if row['proba']>0.5:\n",
    "    df1['y_score'].iloc[indx] = 1\n",
    "cm,acc_score = confusion_matrix(df1)\n",
    "print('Confusion-Matrix :\\n',cm,'\\n')\n",
    "print('f1_score : ',f1_score(df1),'\\n')\n",
    "print('AUC_Score :', AUC_score(df1),'\\n')\n",
    "print('Accuraccy_Score :',acc_score,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9xpOv_cEytM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Compute performance metrics for the given data 5_b.csv\n",
    "\n",
    "    Note 1: in this data you can see number of positive points << number of negatives points\n",
    "    Note 2: use pandas or numpy to read the data from 5_b.csv\n",
    "    Note 3: you need to derive the class labels from given score\n",
    "$y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "a. Compute Confusion Matrix \n",
    "\n",
    "b. Compute F1 Score \n",
    "\n",
    "c. Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then\n",
    "   use numpy.trapz(tpr_array, fpr_array) \n",
    "   \n",
    "    href='https://stackoverflow.com/q/53603376/4084039 https://stackoverflow.com/q/53603376/4084039\n",
    "    href='https://stackoverflow.com/a/39678975/4084039 https://stackoverflow.com/a/39678975/4084039\n",
    "\n",
    "\n",
    "d. Compute Accuracy Score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "hnDD3PIdyYwj",
    "outputId": "12b1fd7b-ee01-4891-e14b-c580b61d16f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion-Matrix :\n",
      " [[9761   45]\n",
      " [ 239   55]] \n",
      "\n",
      "precision : 0.1870748299319728 \n",
      "\n",
      "recall : 0.55 \n",
      "\n",
      "f1_score :  0.2791878172588833 \n",
      "\n",
      "10100 10100\n",
      "AUC_Score : 0.9376570000000001 \n",
      "\n",
      "Accuraccy_Score : 0.9718811881188119 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df2=pd.read_csv('/content/gdrive/My Drive/AAIC/Assignments/7.Compute Performance metrics without Sklearn/Copy of 5_b.csv')\n",
    "df2['y_score']=0\n",
    "for indx,row in df2.iterrows():\n",
    "  if row['proba']>0.5:\n",
    "    df2['y_score'].iloc[indx] = 1\n",
    "cm,acc_score = confusion_matrix(df2)\n",
    "print('Confusion-Matrix :\\n',cm,'\\n')\n",
    "print('f1_score : ',f1_score(df2),'\\n')\n",
    "print('AUC_Score :', AUC_score(df2),'\\n')\n",
    "print('Accuraccy_Score :',acc_score,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UhU4F8ZyZw2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II-9fkd-jRU0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8m9zh8HFneW"
   },
   "source": [
    "C. Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric \"A\" for the given data 5_c.csv\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "       Note 1: in this data you can see number of negative points > number of positive points\n",
    "       Note 2: use pandas or numpy to read the data from 5_c.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vJDBv8qsFpF3"
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('/content/gdrive/My Drive/AAIC/Assignments/7.Compute Performance metrics without Sklearn/Copy of 5_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NfLzI7LsFpaa"
   },
   "outputs": [],
   "source": [
    "def Metric_A(df):\n",
    "  x = df.sort_values(by=['prob'],ascending=False,ignore_index=True)\n",
    "  x['y_score'] = 0\n",
    "  #D_fn = dict()\n",
    "  #D_fp = dict()\n",
    "  D_A  = dict()\n",
    "  for i in range(len(x)):\n",
    "    x['y_score'].iloc[i] = 1\n",
    "    d  = dict(x['y_score'].value_counts())\n",
    "    \n",
    "    d1 = dict(x['y'].iloc[:i].value_counts())\n",
    "    #print('fp',d1.get(0,0))\n",
    "    #print('tp',d1.get(1,0))\n",
    "    \n",
    "    d2 = dict(x['y'].iloc[i:].value_counts())\n",
    "    #print('tn',d2.get(0,0))\n",
    "    #print('fn',d2.get(1,0))\n",
    "\n",
    "    A = (500*(d2.get(1,0))) + (100*(d1.get(0,0)))\n",
    "\n",
    "    #D_fn[x['prob'].iloc[i]]= d2.get(1,0)\n",
    "    #D_fp[x['prob'].iloc[i]]= d1.get(0,0)\n",
    "    D_A[x['prob'].iloc[i]]= A\n",
    "  best_threshold = min(D_A,key=D_A.get) # https://stackoverflow.com/questions/3282823/get-the-key-corresponding-to-the-minimum-value-within-a-dictionary\n",
    "\n",
    "  return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_Ty-H4HlFo5_",
    "outputId": "99ade961-2f9c-48ab-ae2c-75ee34015194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold : 0.22987164436159915\n"
     ]
    }
   ],
   "source": [
    "best_threshold = Metric_A(df3)\n",
    "df3['y_score']=0\n",
    "for indx,row in df3.iterrows():\n",
    "  if row['prob']>best_threshold:\n",
    "    df3['y_score'].iloc[indx] = 1\n",
    "df3 = pd.DataFrame(df3)\n",
    "print('best_threshold :',best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "bzmXB5jkFm0_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AILatBDJ5e1l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7JHcnUHFFGB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqYjjr1hdrg2"
   },
   "source": [
    "D. Compute performance metrics(for regression) for the given data 5_d.csv\n",
    "    \n",
    "    Note 2: use pandas or numpy to read the data from 5_d.csv\n",
    "    Note 1: 5_d.csv will having two columns Y and predicted_Y both are real valued features\n",
    "    \n",
    "Compute Mean Square Error \n",
    "\n",
    "Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk\n",
    "\n",
    "Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Jnd87gT7duJE"
   },
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('/content/gdrive/My Drive/AAIC/Assignments/7.Compute Performance metrics without Sklearn/Copy of 5_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "1LVQmVXIeDLt",
    "outputId": "baf6cc47-262e-4592-a7f5-2ec238dea80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error : 177.16569974554707 \n",
      "\n",
      "Mean Absolute % Error : 0.1291202994009687 \n",
      "\n",
      "R-square Error : 0.9563582786990937 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean(((df4['y']-df4['pred'])**2))\n",
    "mape = (abs(df4['pred']-df4['y']).sum(axis=0))/(df4['y'].sum(axis=0))\n",
    "ss_tot = ((df4['y']-(np.mean(df4['y'])))**2).sum(axis=0)\n",
    "ss_res = ((df4['y']-df4['pred'])**2).sum(axis=0)\n",
    "r2_err = 1-(ss_res/ss_tot)\n",
    "\n",
    "print('Mean Square Error :', mse,'\\n')\n",
    "print('Mean Absolute % Error :', mape,'\\n')\n",
    "print('R-square Error :', r2_err,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "w4z8BPEExUCe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "MMZoi_iWxe4s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "j8mprB7BAjIc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3Zjmr-bQAnda"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "tspLk4ojAqDX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84i2pFqOAxEN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7.Compute Performance metrics without Sklearn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
